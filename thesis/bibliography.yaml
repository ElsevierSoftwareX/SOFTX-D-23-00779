---
references:
- id: abadiTensorFlowLargeScaleMachine2016
  abstract: >-
    TensorFlow is an interface for expressing machine learning algorithms, and
    an implementation for executing such algorithms. A computation expressed
    using TensorFlow can be executed with little or no change on a wide variety
    of heterogeneous systems, ranging from mobile devices such as phones and
    tablets up to large-scale distributed systems of hundreds of machines and
    thousands of computational devices such as GPU cards. The system is flexible
    and can be used to express a wide variety of algorithms, including training
    and inference algorithms for deep neural network models, and it has been
    used for conducting research and for deploying machine learning systems into
    production across more than a dozen areas of computer science and other
    fields, including speech recognition, computer vision, robotics, information
    retrieval, natural language processing, geographic information extraction,
    and computational drug discovery. This paper describes the TensorFlow
    interface and an implementation of that interface that we have built at
    Google. The TensorFlow API and a reference implementation were released as
    an open-source package under the Apache 2.0 license in November, 2015 and
    are available at www.tensorflow.org.
  accessed:
    - year: 2020
      month: 6
      day: 21
  author:
    - family: Abadi
      given: Martín
    - family: Agarwal
      given: Ashish
    - family: Barham
      given: Paul
    - family: Brevdo
      given: Eugene
    - family: Chen
      given: Zhifeng
    - family: Citro
      given: Craig
    - family: Corrado
      given: Greg S.
    - family: Davis
      given: Andy
    - family: Dean
      given: Jeffrey
    - family: Devin
      given: Matthieu
    - family: Ghemawat
      given: Sanjay
    - family: Goodfellow
      given: Ian
    - family: Harp
      given: Andrew
    - family: Irving
      given: Geoffrey
    - family: Isard
      given: Michael
    - family: Jia
      given: Yangqing
    - family: Jozefowicz
      given: Rafal
    - family: Kaiser
      given: Lukasz
    - family: Kudlur
      given: Manjunath
    - family: Levenberg
      given: Josh
    - family: Mane
      given: Dan
    - family: Monga
      given: Rajat
    - family: Moore
      given: Sherry
    - family: Murray
      given: Derek
    - family: Olah
      given: Chris
    - family: Schuster
      given: Mike
    - family: Shlens
      given: Jonathon
    - family: Steiner
      given: Benoit
    - family: Sutskever
      given: Ilya
    - family: Talwar
      given: Kunal
    - family: Tucker
      given: Paul
    - family: Vanhoucke
      given: Vincent
    - family: Vasudevan
      given: Vijay
    - family: Viegas
      given: Fernanda
    - family: Vinyals
      given: Oriol
    - family: Warden
      given: Pete
    - family: Wattenberg
      given: Martin
    - family: Wicke
      given: Martin
    - family: Yu
      given: Yuan
    - family: Zheng
      given: Xiaoqiang
  container-title: 'arXiv:1603.04467 [cs]'
  issued:
    - year: 2016
      month: 3
      day: 16
  source: arXiv.org
  title: >-
    TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed
    Systems
  title-short: TensorFlow
  type: article-journal
  URL: 'http://arxiv.org/abs/1603.04467'

- id: agarapDeepLearningUsing2019
  abstract: >-
    We introduce the use of rectified linear units (ReLU) as the classification
    function in a deep neural network (DNN). Conventionally, ReLU is used as an
    activation function in DNNs, with Softmax function as their classification
    function. However, there have been several studies on using a classification
    function other than Softmax, and this study is an addition to those. We
    accomplish this by taking the activation of the penultimate layer $h_{n -
    1}$ in a neural network, then multiply it by weight parameters $\theta$ to
    get the raw scores $o_{i}$. Afterwards, we threshold the raw scores $o_{i}$
    by $0$, i.e. $f(o) = \max(0, o_{i})$, where $f(o)$ is the ReLU function. We
    provide class predictions $\hat{y}$ through argmax function, i.e. argmax
    $f(x)$.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Agarap
      given: Abien Fred
  container-title: 'arXiv:1803.08375 [cs, stat]'
  issued:
    - year: 2019
      month: 2
      day: 7
  source: arXiv.org
  title: Deep Learning using Rectified Linear Units (ReLU)
  type: article-journal
  URL: 'http://arxiv.org/abs/1803.08375'

- id: AIPlatform
  abstract: >-
    A development platform to build AI applications that run on GCP and
    on-premises. Take your ML projects to production, quickly and
    cost-effectively.
  accessed:
    - year: 2020
      month: 6
      day: 22
  container-title: Google Cloud
  language: en
  source: cloud.google.com
  title: AI Platform
  type: webpage
  URL: 'https://cloud.google.com/ai-platform'

- id: akibaOptunaNextgenerationHyperparameter2019
  abstract: >-
    The purpose of this study is to introduce new design-criteria for
    next-generation hyperparameter optimization software. The criteria we
    propose include (1) define-by-run API that allows users to construct the
    parameter search space dynamically, (2) efficient implementation of both
    searching and pruning strategies, and (3) easy-to-setup, versatile
    architecture that can be deployed for various purposes, ranging from
    scalable distributed computing to light-weight experiment conducted via
    interactive interface. In order to prove our point, we will introduce
    Optuna, an optimization software which is a culmination of our effort in the
    development of a next generation optimization software. As an optimization
    software designed with define-by-run principle, Optuna is particularly the
    first of its kind. We will present the design-techniques that became
    necessary in the development of the software that meets the above criteria,
    and demonstrate the power of our new design through experimental results and
    real world applications. Our software is available under the MIT license
    (https://github.com/pfnet/optuna/).
  accessed:
    - year: 2020
      month: 6
      day: 24
  author:
    - family: Akiba
      given: Takuya
    - family: Sano
      given: Shotaro
    - family: Yanase
      given: Toshihiko
    - family: Ohta
      given: Takeru
    - family: Koyama
      given: Masanori
  collection-title: KDD '19
  container-title: >-
    Proceedings of the 25th ACM SIGKDD International Conference on Knowledge
    Discovery & Data Mining
  DOI: 10.1145/3292500.3330701
  event-place: 'Anchorage, AK, USA'
  ISBN: 978-1-4503-6201-6
  issued:
    - year: 2019
      month: 7
      day: 25
  page: 2623–2631
  publisher: Association for Computing Machinery
  publisher-place: 'Anchorage, AK, USA'
  source: ACM Digital Library
  title: 'Optuna: A Next-generation Hyperparameter Optimization Framework'
  title-short: Optuna
  type: paper-conference
  URL: 'https://doi.org/10.1145/3292500.3330701'

- id: albertScienceCaseWide2019
  abstract: >-
    We outline the science motivation for SGSO, the Southern Gamma-Ray Survey
    Observatory. SGSO will be a next-generation wide field-of-view gamma-ray
    survey instrument, sensitive to gamma-rays in the energy range from 100 GeV
    to hundreds of TeV. Its science topics include unveiling galactic and
    extragalactic particle accelerators, monitoring the transient sky at very
    high energies, probing particle physics beyond the Standard Model, and the
    characterization of the cosmic ray flux. SGSO will consist of an air shower
    detector array, located in South America. Due to its location and large
    field of view, SGSO will be complementary to other current and planned
    gamma-ray observatories such as HAWC, LHAASO, and CTA.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Albert
      given: A.
    - family: Alfaro
      given: R.
    - family: Ashkar
      given: H.
    - family: Alvarez
      given: C.
    - family: Álvarez
      given: J.
    - family: Arteaga-Velázquez
      given: J. C.
    - family: Solares
      given: H. A. Ayala
    - family: Arceo
      given: R.
    - family: Bellido
      given: J. A.
    - family: BenZvi
      given: S.
    - family: Bretz
      given: T.
    - family: Brisbois
      given: C. A.
    - family: Brown
      given: A. M.
    - family: Brun
      given: F.
    - family: Caballero-Mora
      given: K. S.
    - family: Carosi
      given: A.
    - family: Carramiñana
      given: A.
    - family: Casanova
      given: S.
    - family: Chadwick
      given: P. M.
    - family: Cotter
      given: G.
    - family: De León
      given: S. Coutiño
    - family: Cristofari
      given: P.
    - family: Dasso
      given: S.
    - family: Fuente
      given: E.
      non-dropping-particle: de la
    - family: Dingus
      given: B. L.
    - family: Desiati
      given: P.
    - family: Salles
      given: F. de O.
    - family: Souza
      given: V.
      non-dropping-particle: de
    - family: Dorner
      given: D.
    - family: Díaz-Vélez
      given: J. C.
    - family: García-González
      given: J. A.
    - family: DuVernois
      given: M. A.
    - family: Di Sciascio
      given: G.
    - family: Engel
      given: K.
    - family: Fleischhack
      given: H.
    - family: Fraija
      given: N.
    - family: Funk
      given: S.
    - family: Glicenstein
      given: J.-F.
    - family: Gonzalez
      given: J.
    - family: González
      given: M. M.
    - family: Goodman
      given: J. A.
    - family: Harding
      given: J. P.
    - family: Haungs
      given: A.
    - family: Hinton
      given: J.
    - family: Hona
      given: B.
    - family: Hoyos
      given: D.
    - family: Huentemeyer
      given: P.
    - family: Iriarte
      given: A.
    - family: Jardin-Blicq
      given: A.
    - family: Joshi
      given: V.
    - family: Kaufmann
      given: S.
    - family: Kawata
      given: K.
    - family: Kunwar
      given: S.
    - family: Lefaucheur
      given: J.
    - family: Lenain
      given: J.-P.
    - family: Link
      given: K.
    - family: López-Coto
      given: R.
    - family: Marandon
      given: V.
    - family: Mariotti
      given: M.
    - family: Martínez-Castro
      given: J.
    - family: Martínez-Huerta
      given: H.
    - family: Mostafá
      given: M.
    - family: Nayerhoda
      given: A.
    - family: Nellen
      given: L.
    - family: Wilhelmi
      given: E. de Oña
    - family: Parsons
      given: R. D.
    - family: Patricelli
      given: B.
    - family: Pichel
      given: A.
    - family: Piel
      given: Q.
    - family: Prandini
      given: E.
    - family: Pueschel
      given: E.
    - family: Procureur
      given: S.
    - family: Reisenegger
      given: A.
    - family: Rivière
      given: C.
    - family: Rodriguez
      given: J.
    - family: Rovero
      given: A. C.
    - family: Rowell
      given: G.
    - family: Ruiz-Velasco
      given: E. L.
    - family: Sandoval
      given: A.
    - family: Santander
      given: M.
    - family: Sako
      given: T.
    - family: Sako
      given: T. K.
    - family: Satalecka
      given: K.
    - family: Schoorlemmer
      given: H.
    - family: Schüssler
      given: F.
    - family: Seglar-Arroyo
      given: M.
    - family: Smith
      given: A. J.
    - family: Spencer
      given: S.
    - family: Surajbali
      given: P.
    - family: Tabachnick
      given: E.
    - family: Taylor
      given: A. M.
    - family: Tibolla
      given: O.
    - family: Torres
      given: I.
    - family: Vallage
      given: B.
    - family: Viana
      given: A.
    - family: Watson
      given: J. J.
    - family: Weisgarber
      given: T.
    - family: Werner
      given: F.
    - family: White
      given: R.
    - family: Wischnewski
      given: R.
    - family: Yang
      given: R.
    - family: Zepeda
      given: A.
    - family: Zhou
      given: H.
  container-title: 'arXiv:1902.08429 [astro-ph]'
  issued:
    - year: 2019
      month: 2
      day: 22
  source: arXiv.org
  title: >-
    Science Case for a Wide Field-of-View Very-High-Energy Gamma-Ray Observatory
    in the Southern Hemisphere
  type: article-journal
  URL: 'http://arxiv.org/abs/1902.08429'

- id: AmazonSageMaker
  abstract: >-
    Amazon SageMaker es un servicio completamente administrado que brinda a
    todos los científicos de datos y desarrolladores la capacidad de crear,
    entrenar e implementar modelos de aprendizaje automático de forma rápida.
    SageMaker elimina las tareas arduas de cada paso del proceso de aprendizaje
    automático para que sea más fácil crear modelos de alta calidad.
  accessed:
    - year: 2020
      month: 6
      day: 22
  container-title: 'Amazon Web Services, Inc.'
  language: es-ES
  source: aws.amazon.com
  title: Amazon SageMaker
  type: webpage
  URL: 'https://aws.amazon.com/es/sagemaker/'

- id: argiroOfflineSoftwareFramework2007
  abstract: >-
    The Pierre Auger Observatory is designed to unveil the nature and the
    origins of the highest energy cosmic rays. The large and geographically
    dispersed collaboration of physicists and the wide-ranging collection of
    simulation and reconstruction tasks pose some special challenges for the
    offline analysis software. We have designed and implemented a general
    purpose framework which allows collaborators to contribute algorithms and
    sequencing instructions to build up the variety of applications they
    require. The framework includes machinery to manage these user codes, to
    organize the abundance of user-contributed configuration files, to
    facilitate multi-format file handling, and to provide access to event and
    time-dependent detector information which can reside in various data
    sources. A number of utilities are also provided, including a novel geometry
    package which allows manipulation of abstract geometrical objects
    independent of coordinate system choice. The framework is implemented in
    C++, and takes advantage of object oriented design and common open source
    tools, while keeping the user side simple enough for C++ novices to learn in
    a reasonable time. The distribution system incorporates unit and acceptance
    testing in order to support rapid development of both the core framework and
    contributed user code.
  accessed:
    - year: 2020
      month: 6
      day: 25
  author:
    - family: Argiro
      given: S.
    - family: Barroso
      given: S. L. C.
    - family: Gonzalez
      given: J.
    - family: Nellen
      given: L.
    - family: Paul
      given: T.
    - family: Porter
      given: T. A.
    - family: Prado Jr.
      given: L.
    - family: Roth
      given: M.
    - family: Ulrich
      given: R.
    - family: Veberic
      given: D.
  container-title: >-
    Nuclear Instruments and Methods in Physics Research Section A: Accelerators,
    Spectrometers, Detectors and Associated Equipment
  container-title-short: >-
    Nuclear Instruments and Methods in Physics Research Section A: Accelerators,
    Spectrometers, Detectors and Associated Equipment
  DOI: 10.1016/j.nima.2007.07.010
  ISSN: 01689002
  issue: '3'
  issued:
    - year: 2007
      month: 10
  page: 1485-1496
  source: arXiv.org
  title: The Offline Software Framework of the Pierre Auger Observatory
  type: article-journal
  URL: 'http://arxiv.org/abs/0707.1652'
  volume: '580'

- id: arnoldAutomatingAIOperations2020
  abstract: >-
    Today's AI deployments often require significant human involvement and skill
    in the operational stages of the model lifecycle, including pre-release
    testing, monitoring, problem diagnosis and model improvements. We present a
    set of enabling technologies that can be used to increase the level of
    automation in AI operations, thus lowering the human effort required. Since
    a common source of human involvement is the need to assess the performance
    of deployed models, we focus on technologies for performance prediction and
    KPI analysis and show how they can be used to improve automation in the key
    stages of a typical AI operations pipeline.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Arnold
      given: Matthew
    - family: Boston
      given: Jeffrey
    - family: Desmond
      given: Michael
    - family: Duesterwald
      given: Evelyn
    - family: Elder
      given: Benjamin
    - family: Murthi
      given: Anupama
    - family: Navratil
      given: Jiri
    - family: Reimer
      given: Darrell
  container-title: 'arXiv:2003.12808 [cs]'
  issued:
    - year: 2020
      month: 3
      day: 28
  source: arXiv.org
  title: Towards Automating the AI Operations Lifecycle
  type: article-journal
  URL: 'http://arxiv.org/abs/2003.12808'

- id: ArtificialIntelligenceFaces
  accessed:
    - year: 2020
      month: 6
      day: 21
  title: Artificial intelligence faces reproducibility crisis | Science
  type: webpage
  URL: 'https://science.sciencemag.org/content/359/6377/725.summary'

- id: assisLATTESNovelDetector2018
  abstract: >-
    The Large Array Telescope for Tracking Energetic Sources (LATTES), is a
    novel concept for an array of hybrid EAS array detectors, composed of a
    Resistive Plate Counter array coupled to a Water Cherenkov Detector, planned
    to cover gamma rays from less than 100 GeV up to 100 TeVs. This experiment,
    to be installed at high altitude in South America, could cover the existing
    gap in sensitivity between satellite and ground arrays. The low energy
    threshold, large duty cycle and wide field of view of LATTES makes it a
    powerful tool to detect transient phenomena and perform long term
    observations of variable sources. Moreover, given its characteristics, it
    would be fully complementary to the planned Cherenkov Telescope Array (CTA)
    as it would be able to issue alerts. In this talk, a description of its main
    features and capabilities, as well as results on its expected performance,
    and sensitivity, will be presented.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Assis
      given: P.
    - family: Almeida
      given: U. Barres
      non-dropping-particle: de
    - family: Blanco
      given: A.
    - family: Conceição
      given: R.
    - family: Piazzoli
      given: B. D'Ettore
    - family: De Angelis
      given: A.
    - family: Doro
      given: M.
    - family: Fonte
      given: P.
    - family: Lopes
      given: L.
    - family: Matthiae
      given: G.
    - family: Pimenta
      given: M.
    - family: Shellard
      given: R.
    - family: Tomé
      given: B.
  container-title: 'arXiv:1709.09624 [astro-ph, physics:hep-ex, physics:physics]'
  issued:
    - year: 2018
      month: 4
      day: 30
  source: arXiv.org
  title: >-
    LATTES: a novel detector concept for a gamma-ray experiment in the Southern
    hemisphere
  title-short: LATTES
  type: article-journal
  URL: 'http://arxiv.org/abs/1709.09624'

- id: assuncaoAutomaticDesignArtificial2019
  abstract: >-
    The goal of this work is to investigate the possibility of improving current
    gamma/hadron discrimination based on their shower patterns recorded on the
    ground. To this end we propose the use of Convolutional Neural Networks
    (CNNs) for their ability to distinguish patterns based on automatically
    designed features. In order to promote the creation of CNNs that properly
    uncover the hidden patterns in the data, and at same time avoid the burden
    of hand-crafting the topology and learning hyper-parameters we resort to
    NeuroEvolution; in particular we use Fast-DENSER++, a variant of Deep
    Evolutionary Network Structured Representation. The results show that the
    best CNN generated by Fast-DENSER++ improves by a factor of 2 when compared
    with the results reported by classic statistical approaches. Additionally,
    we experiment ensembling the 10 best generated CNNs, one from each of the
    evolutionary runs; the ensemble leads to an improvement by a factor of 2.3.
    These results show that it is possible to improve the gamma/hadron
    discrimination based on CNNs that are automatically generated and are
    trained with instances of the ground impact patterns.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Assunção
      given: Filipe
    - family: Correia
      given: João
    - family: Conceição
      given: Rúben
    - family: Pimenta
      given: Mário
    - family: Tomé
      given: Bernardo
    - family: Lourenço
      given: Nuno
    - family: Machado
      given: Penousal
  container-title: IEEE Access
  container-title-short: IEEE Access
  DOI: 10.1109/ACCESS.2019.2933947
  ISSN: 2169-3536
  issued:
    - year: 2019
  page: 110531-110540
  source: arXiv.org
  title: Automatic Design of Artificial Neural Networks for Gamma-Ray Detection
  type: article-journal
  URL: 'http://arxiv.org/abs/1905.03532'
  volume: '7'

- id: AutoencoderBetaVAE2018
  abstract: >-
    Autocoders are a family of neural network models aiming to learn compressed
    latent variables of high-dimensional data. Starting from the basic autocoder
    model, this post reviews several variations, including denoising, sparse,
    and contractive autoencoders, and then Variational Autoencoder (VAE) and its
    modification beta-VAE.
  accessed:
    - year: 2020
      month: 6
      day: 18
  container-title: Lil'Log
  issued:
    - year: 2018
      month: 8
      day: 12
  language: en
  source: lilianweng.github.io
  title: From Autoencoder to Beta-VAE
  type: webpage
  URL: 'https://lilianweng.github.io/2018/08/12/from-autoencoder-to-beta-vae.html'

- id: AutoencodersClassifiers
  accessed:
    - year: 2020
      month: 6
      day: 18
  title: Autoencoders as Classifiers
  type: webpage
  URL: 'https://radicalrafi.github.io/posts/autoencoders-as-classifiers/'

- id: AWSCloudComputing
  abstract: >-
    Descubra la infraestructura de la nube que ofrece Amazon Web Services,
    servicios de alojamiento en la nube, servicios informática en la nube y
    servicios en la nube para empresas. Regístrate gratis y paga sólo por el
    uso.
  accessed:
    - year: 2020
      month: 6
      day: 22
  container-title: 'Amazon Web Services, Inc.'
  language: es-ES
  source: aws.amazon.com
  title: AWS | Cloud Computing - Servicios de informática en la nube
  type: webpage
  URL: 'https://aws.amazon.com/es/'

- id: AzureMachineLearning
  abstract: >-
    Cree e implemente modelos de aprendizaje automático de manera simplificada
    con Azure Machine Learning Service. Mejore la accesibilidad al aprendizaje
    automático con características automatizadas.
  accessed:
    - year: 2020
      month: 6
      day: 22
  language: es
  source: azure.microsoft.com
  title: Azure Machine Learning | Microsoft Azure
  type: webpage
  URL: 'https://azure.microsoft.com/es-es/services/machine-learning/'

- id: baker500ScientistsLift2016
  abstract: Survey sheds light on the ‘crisis’ rocking research.
  accessed:
    - year: 2020
      month: 6
      day: 19
  author:
    - family: Baker
      given: Monya
  container-title: Nature News
  DOI: 10.1038/533452a
  issue: '7604'
  issued:
    - year: 2016
      month: 5
      day: 26
  language: en
  page: '452'
  section: News Feature
  source: www.nature.com
  title: '1,500 scientists lift the lid on reproducibility'
  type: article-journal
  URL: >-
    http://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970
  volume: '533'

- id: baldi2012autoencoders
  author:
    - family: Baldi
      given: Pierre
  container-title: Proceedings of ICML workshop on unsupervised and transfer learning
  issued:
    - year: 2012
  page: 37–49
  title: 'Autoencoders, unsupervised learning, and deep architectures'
  type: paper-conference

- id: bar-hillelPresentStatusAutomatic1960
  abstract: >-
    Machine translation (MT) has become a multimillion dollar affair. Fully
    automatic, high quality translation is not a reasonable goal, not even for
    scientific texts. This chapter surveys the situations where translation
    involved has to be of high quality. A human translator, in order to arrive
    at his/her high quality output, is obliged to make intelligent use of extra
    linguistic knowledge that sometimes has to be of considerable breadth and
    depth. Reasonable goals are either fully automatic, low quality translation
    or partly automatic, or high quality translation. Full automation of the
    translation process is incompatible with high quality. The two possible
    directions where a compromise could be struck are sacrificing quality or
    reducing the self-sufficiency of the machine output. There are many
    situations where less than high quality machine output is satisfactory.
    However, when the aim of MT is lowered to that of high quality translation
    by a machine-post-editor partnership, the decisive problem becomes to
    determine the region of optimality in the continuum of possible divisions of
    labor. The exact position of this region will be a function of the state of
    linguistic analysis where the languages involved are submitted. With
    machine-time/efficiency becoming cheaper and human time becoming more
    expensive, continuous efforts will be made to push this region in the
    direction of reducing the human element. However, there is no good reason to
    assume that this region can be pushed to the end of the line, certainly not
    in the near future.
  accessed:
    - year: 2020
      month: 6
      day: 21
  author:
    - family: Bar-Hillel
      given: Yehoshua
  container-title: Advances in Computers
  DOI: 10.1016/S0065-2458(08)60607-5
  editor:
    - family: Alt
      given: Franz L.
  issued:
    - year: 1960
      month: 1
      day: 1
  language: en
  page: 91-163
  publisher: Elsevier
  source: ScienceDirect
  title: >-
    The Present Status of Automatic Translation of Languages**This article was
    prepared with the sponsorship of the Informations Systems Branch, Office of
    Naval Research, under Contract NR 049130. Reproduction as a whole or in part
    for the purposes of the U. S. Government is permitted.
  type: chapter
  URL: 'http://www.sciencedirect.com/science/article/pii/S0065245808606075'
  volume: '1'

- id: bengioDeepLearning2017
  abstract: >-
    An introduction to a broad range of topics in deep learning, covering
    mathematical and conceptual background, deep learning techniques used in
    industry, and research perspectives. "Written by three experts in the field,
    Deep Learning is the only comprehensive book on the subject." -Elon Musk,
    cochair of OpenAI; cofounder and CEO of Tesla and SpaceX Deep learning is a
    form of machine learning that enables computers to learn from experience and
    understand the world in terms of a hierarchy of concepts. Because the
    computer gathers knowledge from experience, there is no need for a human
    computer operator to formally specify all the knowledge that the computer
    needs. The hierarchy of concepts allows the computer to learn complicated
    concepts by building them out of simpler ones; a graph of these hierarchies
    would be many layers deep. This book introduces a broad range of topics in
    deep learning. The text offers mathematical and conceptual background,
    covering relevant concepts in linear algebra, probability theory and
    information theory, numerical computation, and machine learning. It
    describes deep learning techniques used by practitioners in industry,
    including deep feedforward networks, regularization, optimization
    algorithms, convolutional networks, sequence modeling, and practical
    methodology; and it surveys such applications as natural language
    processing, speech recognition, computer vision, online recommendation
    systems, bioinformatics, and videogames. Finally, the book offers research
    perspectives, covering such theoretical topics as linear factor models,
    autoencoders, representation learning, structured probabilistic models,
    Monte Carlo methods, the partition function, approximate inference, and deep
    generative models. Deep Learning can be used by undergraduate or graduate
    students planning careers in either industry or research, and by software
    engineers who want to begin using deep learning in their products or
    platforms. A website offers supplementary material for both readers and
    instructors.
  author:
    - family: Bengio
      given: Yoshua
  event-place: 'Cambridge, Massachusetts'
  ISBN: 978-0-262-03561-3
  issued:
    - year: 2017
      month: 1
      day: 3
  language: Inglés
  number-of-pages: '800'
  publisher: MIT Press
  publisher-place: 'Cambridge, Massachusetts'
  source: Amazon
  title: Deep Learning
  type: book

- id: bergstraAlgorithmsHyperParameterOptimization2011
  accessed:
    - year: 2020
      month: 6
      day: 25
  author:
    - family: Bergstra
      given: James S.
    - family: Bardenet
      given: Rémi
    - family: Bengio
      given: Yoshua
    - family: Kégl
      given: Balázs
  container-title: Advances in Neural Information Processing Systems 24
  editor:
    - family: Shawe-Taylor
      given: J.
    - family: Zemel
      given: R. S.
    - family: Bartlett
      given: P. L.
    - family: Pereira
      given: F.
    - family: Weinberger
      given: K. Q.
  issued:
    - year: 2011
  page: 2546–2554
  publisher: 'Curran Associates, Inc.'
  source: Neural Information Processing Systems
  title: Algorithms for Hyper-Parameter Optimization
  type: chapter
  URL: >-
    http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf

- id: boehmSystemDSDeclarativeMachine2020
  abstract: >-
    Machine learning (ML) applications become increasingly common in many
    domains. ML systems to execute these workloads include numerical computing
    frameworks and libraries, ML algorithm libraries, and specialized systems
    for deep neural networks and distributed ML. These systems focus primarily
    on efficient model training and scoring. However, the data science process
    is exploratory, and deals with underspecified objectives and a wide variety
    of heterogeneous data sources. Therefore, additional tools are employed for
    data engineering and debugging, which requires boundary crossing,
    unnecessary manual effort, and lacks optimization across the lifecycle. In
    this paper, we introduce SystemDS, an open source ML system for the
    end-to-end data science lifecycle from data integration, cleaning, and
    preparation, over local, distributed, and federated ML model training, to
    debugging and serving. To this end, we aim to provide a stack of declarative
    languages with R-like syntax for the different lifecycle tasks, and users
    with different expertise. We describe the overall system architecture,
    explain major design decisions (motivated by lessons learned from Apache
    SystemML), and discuss key features and research directions. Finally, we
    provide preliminary results that show the potential of end-to-end lifecycle
    optimization.
  author:
    - family: Boehm
      given: Matthias
    - family: Antonov
      given: Iulian
    - family: Dokter
      given: Mark
    - family: Ginthoer
      given: Robert
    - family: Innerebner
      given: Kevin
    - family: Klezin
      given: Florijan
    - family: Lindstädt
      given: Stefanie N.
    - family: Phani
      given: Arnab
    - family: Rath
      given: Benjamin
  container-title: CIDR
  issued:
    - year: 2020
  source: Semantic Scholar
  title: >-
    SystemDS: A Declarative Machine Learning System for the End-to-End Data
    Science Lifecycle
  title-short: SystemDS
  type: article-journal

- id: boettigerIntroductionDockerReproducible2015
  abstract: >-
    As computational work becomes more and more integral to many aspects of
    scientific research, computational reproducibility has become an issue of
    increasing importance to computer systems researchers and domain scientists
    alike. Though computational reproducibility seems more straight forward than
    replicating physical experiments, the complex and rapidly changing nature of
    computer environments makes being able to reproduce and extend such work a
    serious challenge. In this paper, I explore common reasons that code
    developed for one research project cannot be successfully executed or
    extended by subsequent researchers. I review current approaches to these
    issues, including virtual machines and workflow systems, and their
    limitations. I then examine how the popular emerging technology Docker
    combines several areas from systems research - such as operating system
    virtualization, cross-platform portability, modular re-usable elements,
    versioning, and a `DevOps' philosophy, to address these challenges. I
    illustrate this with several examples of Docker use with a focus on the R
    statistical environment.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Boettiger
      given: Carl
  container-title: ACM SIGOPS Operating Systems Review
  container-title-short: SIGOPS Oper. Syst. Rev.
  DOI: 10.1145/2723872.2723882
  ISSN: 0163-5980
  issue: '1'
  issued:
    - year: 2015
      month: 1
      day: 20
  page: 71-79
  source: arXiv.org
  title: >-
    An introduction to Docker for reproducible research, with examples from the
    R environment
  type: article-journal
  URL: 'http://arxiv.org/abs/1410.0846'
  volume: '49'

- id: brunROOTObjectOriented1997
  abstract: >-
    The ROOT system in an Object Oriented framework for large scale data
    analysis. ROOT written in C++, contains, among others, an efficient
    hierarchical OO database, a C++ interpreter, advanced statistical analysis
    (multi-dimensional histogramming, fitting, minimization, cluster finding
    algorithms) and visualization tools. The user interacts with ROOT via a
    graphical user interface, the command line or batch scripts. The command and
    scripting language is C++ (using the interpreter) and large scripts can be
    compiled and dynamically linked in. The OO database design has been
    optimized for parallel access (reading as well as writing) by multiple
    processes.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Brun
      given: Rene
    - family: Rademakers
      given: Fons
  collection-title: New Computing Techniques in Physics Research V
  container-title: >-
    Nuclear Instruments and Methods in Physics Research Section A: Accelerators,
    Spectrometers, Detectors and Associated Equipment
  container-title-short: >-
    Nuclear Instruments and Methods in Physics Research Section A: Accelerators,
    Spectrometers, Detectors and Associated Equipment
  DOI: 10.1016/S0168-9002(97)00048-X
  ISSN: 0168-9002
  issue: '1'
  issued:
    - year: 1997
      month: 4
      day: 11
  language: en
  page: 81-86
  source: ScienceDirect
  title: ROOT — An object oriented data analysis framework
  type: article-journal
  URL: 'http://www.sciencedirect.com/science/article/pii/S016890029700048X'
  volume: '389'

- id: chalapathyDeepLearningAnomaly2019
  abstract: >-
    Anomaly detection is an important problem that has been well-studied within
    diverse research areas and application domains. The aim of this survey is
    two-fold, firstly we present a structured and comprehensive overview of
    research methods in deep learning-based anomaly detection. Furthermore, we
    review the adoption of these methods for anomaly across various application
    domains and assess their effectiveness. We have grouped state-of-the-art
    research techniques into different categories based on the underlying
    assumptions and approach adopted. Within each category we outline the basic
    anomaly detection technique, along with its variants and present key
    assumptions, to differentiate between normal and anomalous behavior. For
    each category, we present we also present the advantages and limitations and
    discuss the computational complexity of the techniques in real application
    domains. Finally, we outline open issues in research and challenges faced
    while adopting these techniques.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Chalapathy
      given: Raghavendra
    - family: Chawla
      given: Sanjay
  container-title: 'arXiv:1901.03407 [cs, stat]'
  issued:
    - year: 2019
      month: 1
      day: 23
  source: arXiv.org
  title: 'Deep Learning for Anomaly Detection: A Survey'
  title-short: Deep Learning for Anomaly Detection
  type: article-journal
  URL: 'http://arxiv.org/abs/1901.03407'

- id: chenSequentialVAELSTMAnomaly2019
  abstract: >-
    In order to support stable web-based applications and services, anomalies on
    the IT performance status have to be detected timely. Moreover, the
    performance trend across the time series should be predicted. In this paper,
    we propose SeqVL (Sequential VAE-LSTM), a neural network model based on both
    VAE (Variational Auto-Encoder) and LSTM (Long Short-Term Memory). This work
    is the first attempt to integrate unsupervised anomaly detection and trend
    prediction under one framework. Moreover, this model performs considerably
    better on detection and prediction than VAE and LSTM work alone. On
    unsupervised anomaly detection, SeqVL achieves competitive experimental
    results compared with other state-of-the-art methods on public datasets. On
    trend prediction, SeqVL outperforms several classic time series prediction
    models in the experiments of the public dataset.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Chen
      given: Run-Qing
    - family: Shi
      given: Guang-Hui
    - family: Zhao
      given: Wan-Lei
    - family: Liang
      given: Chang-Hui
  container-title: 'arXiv:1910.03818 [cs, stat]'
  issued:
    - year: 2019
      month: 10
      day: 10
  source: arXiv.org
  title: Sequential VAE-LSTM for Anomaly Detection on Time Series
  type: article-journal
  URL: 'http://arxiv.org/abs/1910.03818'

- id: chenXGBoostScalableTree2016
  abstract: >-
    Tree boosting is a highly effective and widely used machine learning method.
    In this paper, we describe a scalable end-to-end tree boosting system called
    XGBoost, which is used widely by data scientists to achieve state-of-the-art
    results on many machine learning challenges. We propose a novel
    sparsity-aware algorithm for sparse data and weighted quantile sketch for
    approximate tree learning. More importantly, we provide insights on cache
    access patterns, data compression and sharding to build a scalable tree
    boosting system. By combining these insights, XGBoost scales beyond billions
    of examples using far fewer resources than existing systems.
  accessed:
    - year: 2020
      month: 6
      day: 21
  author:
    - family: Chen
      given: Tianqi
    - family: Guestrin
      given: Carlos
  collection-title: KDD '16
  container-title: >-
    Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge
    Discovery and Data Mining
  DOI: 10.1145/2939672.2939785
  event-place: 'San Francisco, California, USA'
  ISBN: 978-1-4503-4232-2
  issued:
    - year: 2016
      month: 8
      day: 13
  page: 785–794
  publisher: Association for Computing Machinery
  publisher-place: 'San Francisco, California, USA'
  source: ACM Digital Library
  title: 'XGBoost: A Scalable Tree Boosting System'
  title-short: XGBoost
  type: paper-conference
  URL: 'https://doi.org/10.1145/2939672.2939785'

- id: chirigatiCollaborativeApproachComputational2016
  abstract: >-
    Although a standard in natural science, reproducibility has been only
    episodically applied in experimental computer science. Scientific papers
    often present a large number of tables, plots and pictures that summarize
    the obtained results, but then loosely describe the steps taken to derive
    them. Not only can the methods and the implementation be complex, but also
    their configuration may require setting many parameters and/or depend on
    particular system configurations. While many researchers recognize the
    importance of reproducibility, the challenge of making it happen often
    outweigh the benefits. Fortunately, a plethora of reproducibility solutions
    have been recently designed and implemented by the community. In particular,
    packaging tools (e.g., ReproZip) and virtualization tools (e.g., Docker) are
    promising solutions towards facilitating reproducibility for both authors
    and reviewers. To address the incentive problem, we have implemented a new
    publication model for the Reproducibility Section of Information Systems
    Journal. In this section, authors submit a reproducibility paper that
    explains in detail the computational assets from a previous published
    manuscript in Information Systems.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Chirigati
      given: Fernando
    - family: Capone
      given: Rebecca
    - family: Shasha
      given: Dennis
    - family: Rampin
      given: Remi
    - family: Freire
      given: Juliana
  container-title: Information Systems
  container-title-short: Information Systems
  DOI: 10.1016/j.is.2016.03.002
  ISSN: 03064379
  issued:
    - year: 2016
      month: 7
  page: 95-97
  source: arXiv.org
  title: A Collaborative Approach to Computational Reproducibility
  type: article-journal
  URL: 'http://arxiv.org/abs/1709.01154'
  volume: '59'

- id: clevertFastAccurateDeep2016
  abstract: >-
    We introduce the "exponential linear unit" (ELU) which speeds up learning in
    deep neural networks and leads to higher classification accuracies. Like
    rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs
    (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for
    positive values. However, ELUs have improved learning characteristics
    compared to the units with other activation functions. In contrast to ReLUs,
    ELUs have negative values which allows them to push mean unit activations
    closer to zero like batch normalization but with lower computational
    complexity. Mean shifts toward zero speed up learning by bringing the normal
    gradient closer to the unit natural gradient because of a reduced bias shift
    effect. While LReLUs and PReLUs have negative values, too, they do not
    ensure a noise-robust deactivation state. ELUs saturate to a negative value
    with smaller inputs and thereby decrease the forward propagated variation
    and information. Therefore, ELUs code the degree of presence of particular
    phenomena in the input, while they do not quantitatively model the degree of
    their absence. In experiments, ELUs lead not only to faster learning, but
    also to significantly better generalization performance than ReLUs and
    LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks
    significantly outperform ReLU networks with batch normalization while batch
    normalization does not improve ELU networks. ELU networks are among the top
    10 reported CIFAR-10 results and yield the best published result on
    CIFAR-100, without resorting to multi-view evaluation or model averaging. On
    ImageNet, ELU networks considerably speed up learning compared to a ReLU
    network with the same architecture, obtaining less than 10% classification
    error for a single crop, single model network.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Clevert
      given: Djork-Arné
    - family: Unterthiner
      given: Thomas
    - family: Hochreiter
      given: Sepp
  container-title: 'arXiv:1511.07289 [cs]'
  issued:
    - year: 2016
      month: 2
      day: 22
  source: arXiv.org
  title: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)
  type: article-journal
  URL: 'http://arxiv.org/abs/1511.07289'

- id: collbergMeasuringReproducibilityComputer
  abstract: >-
    We describe a study into the willingness of Computer Systems researchers to
    share their code and data. We ﬁnd that . . . . We also propose a novel
    sharing speciﬁcation scheme that will require researchers to specify the
    level of reproducibility that reviewers and readers can assume from a paper
    either submitted for publication, or published.
  author:
    - family: Collberg
      given: Christian
    - family: Moraila
      given: Gina
    - family: Shankaran
      given: Akash
    - family: Shi
      given: Zuoming
    - family: Warren
      given: Alex M
  language: en
  page: '37'
  source: Zotero
  title: Measuring Reproducibility in Computer Systems Research
  type: article-journal

- id: collinsDeliveringVisionMLOps
  author:
    - family: Collins
      given: Jon
  language: en
  page: '33'
  source: Zotero
  title: Delivering on the Vision of MLOps
  type: article-journal

- id: CometBuildBetter
  accessed:
    - year: 2020
      month: 6
      day: 22
  language: en-US
  source: www.comet.ml
  title: Comet – Build better models faster!
  type: post-weblog
  URL: 'https://www.comet.ml/site/'

- id: DataScienceCollaboration
  abstract: >-
    Neptune is an experiment tracking tool bringing organization and
    collaboration to data science projects. Exploratory notebooks, model
    training runs, code, hyperparameters, metrics, data versions, results
    exploration visualizations and more. Everything is safely stored, ready to
    be analyzed, shared and discussed with your team.
  accessed:
    - year: 2020
      month: 6
      day: 22
  container-title: neptune.ai
  language: en-US
  source: neptune.ai
  title: Data science collaboration hub.
  type: webpage
  URL: 'https://neptune.ai/'

- id: degrangeIntroductionHighenergyGammaray2015
  abstract: >-
    The present issue is the first of of a two-volume review devoted to
    gamma-ray astronomy above 100 MeV which has witnessed considerable progress
    over the last 20 years. The motivations for research in this area are
    explained, the follow-on articles of these two thematic issues are
    introduced and a brief history of the field is given.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Degrange
      given: Bernard
    - family: Fontaine
      given: Gérard
  container-title: Comptes Rendus Physique
  container-title-short: Comptes Rendus Physique
  DOI: 10.1016/j.crhy.2015.07.003
  ISSN: '16310705'
  issue: 6-7
  issued:
    - year: 2015
      month: 8
  page: 587-599
  source: arXiv.org
  title: Introduction to high-energy gamma-ray astronomy
  type: article-journal
  URL: 'http://arxiv.org/abs/1604.05488'
  volume: '16'

- id: DessaossAtlas2020
  abstract: 'An Open Source, Self-Hosted Platform For Applied Deep Learning Development'
  accessed:
    - year: 2020
      month: 6
      day: 18
  genre: Python
  issued:
    - year: 2020
      month: 6
      day: 18
  original-date:
    - year: 2020
      month: 2
      day: 24
  publisher: Dessa - Open Source
  source: GitHub
  title: dessa-oss/atlas
  type: book
  URL: 'https://github.com/dessa-oss/atlas'

- id: EdgeOrg
  accessed:
    - year: 2020
      month: 6
      day: 21
  title: Edge.org
  type: webpage
  URL: 'https://www.edge.org/response-detail/25340'

- id: freireComputationalReproducibilityStateoftheart2012
  abstract: >-
    Computational experiments have become an integral part of the scientific
    method, but reproducing, archiving, and querying them is still a challenge.
    The first barrier to a wider adoption is the fact that it is hard both for
    authors to derive a compendium that encapsulates all the components needed
    to reproduce a result and for reviewers to verify the results. In this
    tutorial, we will present a series of guidelines and, through hands-on
    examples, review existing tools to help authors create of reproducible
    results. We will also outline open problems and new directions for
    database-related research having to do with querying computational
    experiments.
  accessed:
    - year: 2020
      month: 6
      day: 21
  author:
    - family: Freire
      given: Juliana
    - family: Bonnet
      given: Philippe
    - family: Shasha
      given: Dennis
  collection-title: SIGMOD '12
  container-title: >-
    Proceedings of the 2012 ACM SIGMOD International Conference on Management of
    Data
  DOI: 10.1145/2213836.2213908
  event-place: 'Scottsdale, Arizona, USA'
  ISBN: 978-1-4503-1247-9
  issued:
    - year: 2012
      month: 5
      day: 20
  page: 593–596
  publisher: Association for Computing Machinery
  publisher-place: 'Scottsdale, Arizona, USA'
  source: ACM Digital Library
  title: >-
    Computational reproducibility: state-of-the-art, challenges, and database
    research opportunities
  title-short: Computational reproducibility
  type: paper-conference
  URL: 'https://doi.org/10.1145/2213836.2213908'

- id: freireReproducibilityDataOrientedExperiments2016
  accessed:
    - year: 2020
      month: 6
      day: 21
  author:
    - family: Freire
      given: Juliana
    - family: Fuhr
      given: Norbert
    - family: Rauber
      given: Andreas
  container-title: Dagstuhl Reports
  DOI: 10.4230/DagRep.6.1.108
  editor:
    - family: Freire
      given: Juliana
    - family: Fuhr
      given: Norbert
    - family: Rauber
      given: Andreas
  event-place: 'Dagstuhl, Germany'
  ISSN: 2192-5283
  issue: '1'
  issued:
    - year: 2016
  page: 108–159
  publisher: Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik
  publisher-place: 'Dagstuhl, Germany'
  source: Dagstuhl Research Online Publication Server
  title: >-
    Reproducibility of Data-Oriented Experiments in e-Science (Dagstuhl Seminar
    16041)
  type: article-journal
  URL: 'http://drops.dagstuhl.de/opus/volltexte/2016/5817'
  volume: '6'

- id: geronHandsOnMachineLearning2019
  abstract: >-
    Through a series of recent breakthroughs, deep learning has boosted the
    entire field of machine learning. Now, even programmers who know close to
    nothing about this technology can use simple, efficient tools to implement
    programs capable of learning from data. This practical book shows you how.By
    using concrete examples, minimal theory, and two production-ready Python
    frameworks—Scikit-Learn and TensorFlow—author Aurélien Géron helps you gain
    an intuitive understanding of the concepts and tools for building
    intelligent systems. You’ll learn a range of techniques, starting with
    simple linear regression and progressing to deep neural networks. With
    exercises in each chapter to help you apply what you’ve learned, all you
    need is programming experience to get started.Explore the machine learning
    landscape, particularly neural netsUse Scikit-Learn to track an example
    machine-learning project end-to-endExplore several training models,
    including support vector machines, decision trees, random forests, and
    ensemble methodsUse the TensorFlow library to build and train neural
    netsDive into neural net architectures, including convolutional nets,
    recurrent nets, and deep reinforcement learningLearn techniques for training
    and scaling deep neural nets
  author:
    - family: Géron
      given: Aurélien
  edition: 'Edición: 2'
  issued:
    - year: 2019
      month: 9
      day: 5
  language: Inglés
  number-of-pages: '849'
  publisher: O'Reilly Media
  source: Amazon
  title: >-
    Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow:
    Concepts, Tools, and Techniques to Build Intelligent Systems
  title-short: 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow'
  type: book

- id: gibneyThisAIResearcher2019
  abstract: >-
    Joelle Pineau is leading an effort to encourage artificial-intelligence
    researchers to open up their code.
  accessed:
    - year: 2020
      month: 6
      day: 21
  author:
    - family: Gibney
      given: Elizabeth
  container-title: Nature
  DOI: 10.1038/d41586-019-03895-5
  issue: '7788'
  issued:
    - year: 2019
      month: 12
      day: 19
  language: en
  number: '7788'
  page: 14-14
  publisher: Nature Publishing Group
  source: www.nature.com
  title: This AI researcher is trying to ward off a reproducibility crisis
  type: article-journal
  URL: 'https://www.nature.com/articles/d41586-019-03895-5'
  volume: '577'

- id: guillenDeepLearningTechniques2019
  abstract: >-
    Deep neural networks are a powerful technique that have found ample
    applications in several branches of Physics. In this work, we apply machine
    learning algorithms to a specific problem of Cosmic Ray Physics: the
    estimation of the muon content of extensive air showers when measured at the
    ground. As a working case, we explore the performance of a deep neural
    network applied to the signals recorded by the water-Cherenkov detectors of
    the Surface Detector Array of the Pierre Auger Observatory. We apply deep
    learning architectures to large sets of simulated data. The inner structure
    of the neural network is optimized through the use of genetic algorithms. To
    obtain a prediction of the recorded muon signal in each individual detector,
    we train neural networks with a mixed sample of light, intermediate and
    heavy nuclei. When true and predicted signals are compared at detector
    level, the primary values of the Pearson correlation coefficients are above
    95\%. The relative errors of the predicted muon signals are below 10\% and
    do not depend on the event energy, zenith angle, total signal size, distance
    range or the hadronic model used to generate the events.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Guillen
      given: A.
    - family: Bueno
      given: A.
    - family: Carceller
      given: J. M.
    - family: Martinez-Velazquez
      given: J. C.
    - family: Rubio
      given: G.
    - family: Peixoto
      given: C. J. Todero
    - family: Sanchez-Lucas
      given: P.
  container-title: Astroparticle Physics
  container-title-short: Astroparticle Physics
  DOI: 10.1016/j.astropartphys.2019.03.001
  ISSN: 09276505
  issued:
    - year: 2019
      month: 9
  page: 12-22
  source: arXiv.org
  title: Deep learning techniques applied to the physics of extensive air showers
  type: article-journal
  URL: 'http://arxiv.org/abs/1807.09024'
  volume: '111'

- id: gulliDeepLearningKeras2017
  abstract: >-
    Get to grips with the basics of Keras to implement fast and efficient
    deep-learning modelsAbout This BookImplement various deep-learning
    algorithms in Keras and see how deep-learning can be used in gamesSee how
    various deep-learning models and practical use-cases can be implemented
    using KerasA practical, hands-on guide with real-world examples to give you
    a strong foundation in KerasWho This Book Is ForIf you are a data scientist
    with experience in machine learning or an AI programmer with some exposure
    to neural networks, you will find this book a useful entry point to
    deep-learning with Keras. A knowledge of Python is required for this
    book.What You Will LearnOptimize step-by-step functions on a large neural
    network using the Backpropagation AlgorithmFine-tune a neural network to
    improve the quality of resultsUse deep learning for image and audio
    processingUse Recursive Neural Tensor Networks (RNTNs) to outperform
    standard word embedding in special casesIdentify problems for which
    Recurrent Neural Network (RNN) solutions are suitableExplore the process
    required to implement AutoencodersEvolve a deep neural network using
    reinforcement learningIn DetailThis book starts by introducing you to
    supervised learning algorithms such as simple linear regression, the
    classical multilayer perceptron and more sophisticated deep convolutional
    networks. You will also explore image processing with recognition of hand
    written digit images, classification of images into different categories,
    and advanced objects recognition with related image annotations. An example
    of identification of salient points for face detection is also provided.
    Next you will be introduced to Recurrent Networks, which are optimized for
    processing sequence data such as text, audio or time series. Following that,
    you will learn about unsupervised learning algorithms such as Autoencoders
    and the very popular Generative Adversarial Networks (GAN). You will also
    explore non-traditional uses of neural networks as Style Transfer.Finally,
    you will look at Reinforcement Learning and its application to AI game
    playing, another popular direction of research and application of neural
    networks.Style and approachThis book is an easy-to-follow guide full of
    examples and real-world applications to help you gain an in-depth
    understanding of Keras. This book will showcase more than twenty working
    Deep Neural Networks coded in Python using Keras.
  author:
    - family: Gulli
      given: Antonio
    - family: Pal
      given: Sujit
  ISBN: 978-1-78712-903-0
  issued:
    - year: 2017
      month: 4
      day: 26
  language: en
  number-of-pages: '310'
  publisher: Packt Publishing Ltd
  source: Google Books
  title: Deep Learning with Keras
  type: book

- id: headExtentConsequencesPHacking2015
  abstract: >-
    A focus on novel, confirmatory, and statistically significant results leads
    to substantial bias in the scientific literature. One type of bias, known as
    “p-hacking,” occurs when researchers collect or select data or statistical
    analyses until nonsignificant results become significant. Here, we use
    text-mining to demonstrate that p-hacking is widespread throughout science.
    We then illustrate how one can test for p-hacking when performing a
    meta-analysis and show that, while p-hacking is probably common, its effect
    seems to be weak relative to the real effect sizes being measured. This
    result suggests that p-hacking probably does not drastically alter
    scientific consensuses drawn from meta-analyses.
  accessed:
    - year: 2020
      month: 6
      day: 21
  author:
    - family: Head
      given: Megan L.
    - family: Holman
      given: Luke
    - family: Lanfear
      given: Rob
    - family: Kahn
      given: Andrew T.
    - family: Jennions
      given: Michael D.
  container-title: PLOS Biology
  container-title-short: PLOS Biology
  DOI: 10.1371/journal.pbio.1002106
  ISSN: 1545-7885
  issue: '3'
  issued:
    - year: 2015
      month: 3
      day: 13
  language: en
  page: e1002106
  publisher: Public Library of Science
  source: PLoS Journals
  title: The Extent and Consequences of P-Hacking in Science
  type: article-journal
  URL: >-
    https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002106
  volume: '13'

- id: hecht-nielsenIIITheoryBackpropagation1992
  abstract: >-
    This chapter presents a survey of the elementary theory of the basic
    backpropagation neural network architecture, covering the areas of
    architectural design, performance measurement, function approximation
    capability, and learning. The survey includes a formulation of the
    backpropagation neural network architecture to make it a valid neural
    network and a proof that the backpropagation mean squared error function
    exists and is differentiable. Also included in the survey is a theorem
    showing that any L2 function can be implemented to any desired degree of
    accuracy with a three-layer backpropagation neural network. An appendix
    presents a speculative neurophysiological model illustrating the way in
    which the backpropagation neural network architecture might plausibly be
    implemented in the mammalian brain for corticocortical learning between
    nearby regions of cerebral cortex. One of the crucial decisions in the
    design of the backpropagation architecture is the selection of a sigmoidal
    activation function.
  accessed:
    - year: 2020
      month: 6
      day: 22
  author:
    - family: Hecht-nielsen
      given: ROBERT
  container-title: Neural Networks for Perception
  DOI: 10.1016/B978-0-12-741252-8.50010-8
  editor:
    - family: Wechsler
      given: Harry
  ISBN: 978-0-12-741252-8
  issued:
    - year: 1992
      month: 1
      day: 1
  language: en
  page: 65-93
  publisher: Academic Press
  source: ScienceDirect
  title: >-
    III.3 - Theory of the Backpropagation Neural Network**Based on “nonindent”
    by Robert Hecht-Nielsen, which appeared in Proceedings of the International
    Joint Conference on Neural Networks 1, 593–611, June 1989. © 1989 IEEE.
  type: chapter
  URL: 'http://www.sciencedirect.com/science/article/pii/B9780127412528500108'

- id: heckCORSIKAMonteCarlo1998
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Heck
      given: D.
    - family: Knapp
      given: J.
    - family: Capdevielle
      given: J. N.
    - family: Schatz
      given: G.
    - family: Thouw
      given: T.
  issued:
    - year: 1998
      month: 2
  language: en
  source: inspirehep.net
  title: 'CORSIKA: A Monte Carlo code to simulate extensive air showers'
  title-short: CORSIKA
  type: article-journal
  URL: 'https://inspirehep.net/literature/469835'

- id: hewittActorModelComputation2015
  abstract: >-
    The Actor model is a mathematical theory that treats "Actors" as the
    universal primitives of concurrent digital computation. The model has been
    used both as a framework for a theoretical understanding of concurrency, and
    as the theoretical basis for several practical implementations of concurrent
    systems. Unlike previous models of computation, the Actor model was inspired
    by physical laws. It was also influenced by the programming languages Lisp,
    Simula 67 and Smalltalk-72, as well as ideas for Petri Nets,
    capability-based systems and packet switching. The advent of massive
    concurrency through client-cloud computing and many-core computer
    architectures has galvanized interest in the Actor model. Actor technology
    will see significant application for integrating all kinds of digital
    information for individuals, groups, and organizations so their information
    usefully links together. Information integration needs to make use of the
    following information system principles: * Persistence. Information is
    collected and indexed. * Concurrency: Work proceeds interactively and
    concurrently, overlapping in time. * Quasi-commutativity: Information can be
    used regardless of whether it initiates new work or become relevant to
    ongoing work. * Sponsorship: Sponsors provide resources for computation,
    i.e., processing, storage, and communications. * Pluralism: Information is
    heterogeneous, overlapping and often inconsistent. * Provenance: The
    provenance of information is carefully tracked and recorded The Actor Model
    is intended to provide a foundation for inconsistency robust information
    integration
  accessed:
    - year: 2020
      month: 6
      day: 25
  author:
    - family: Hewitt
      given: Carl
  container-title: 'arXiv:1008.1459 [cs]'
  issued:
    - year: 2015
      month: 1
      day: 21
  source: arXiv.org
  title: 'Actor Model of Computation: Scalable Robust Information Systems'
  title-short: Actor Model of Computation
  type: article-journal
  URL: 'http://arxiv.org/abs/1008.1459'

- id: howardFastaiLayeredAPI2020
  abstract: >-
    fastai is a deep learning library which provides practitioners with
    high-level components that can quickly and easily provide state-of-the-art
    results in standard deep learning domains, and provides researchers with
    low-level components that can be mixed and matched to build new approaches.
    It aims to do both things without substantial compromises in ease of use,
    flexibility, or performance. This is possible thanks to a carefully layered
    architecture, which expresses common underlying patterns of many deep
    learning and data processing techniques in terms of decoupled abstractions.
    These abstractions can be expressed concisely and clearly by leveraging the
    dynamism of the underlying Python language and the flexibility of the
    PyTorch library. fastai includes: a new type dispatch system for Python
    along with a semantic type hierarchy for tensors; a GPU-optimized computer
    vision library which can be extended in pure Python; an optimizer which
    refactors out the common functionality of modern optimizers into two basic
    pieces, allowing optimization algorithms to be implemented in 4&ndash;5
    lines of code; a novel 2-way callback system that can access any part of the
    data, model, or optimizer and change it at any point during training; a new
    data block API; and much more. We used this library to successfully create a
    complete deep learning course, which we were able to write more quickly than
    using previous approaches, and the code was more clear. The library is
    already in wide use in research, industry, and teaching.
  accessed:
    - year: 2020
      month: 6
      day: 21
  author:
    - family: Howard
      given: Jeremy
    - family: Gugger
      given: Sylvain
  container-title: Information
  DOI: 10.3390/info11020108
  issue: '2'
  issued:
    - year: 2020
      month: 2
  language: en
  number: '2'
  page: '108'
  publisher: Multidisciplinary Digital Publishing Institute
  source: www.mdpi.com
  title: 'Fastai: A Layered API for Deep Learning'
  title-short: Fastai
  type: article-journal
  URL: 'https://www.mdpi.com/2078-2489/11/2/108'
  volume: '11'

- id: hutsonAIGlossaryArtificial2017
  abstract: >-
    Just what do people mean by artificial intelligence (AI)? The term has never
    had clear boundaries. When it was introduced at a seminal 1956 workshop at
    Dartmouth College, it was taken broadly to mean making a machine behave in
    ways that would be called intelligent if seen in a human. An important
    recent advance in AI has been machine learning, which shows up in
    technologies from spellcheck to self-driving cars and is often carried out
    by computer systems called neural networks. Any discussion of AI is likely
    to include other terms as well. We present a glossary of key words and
    phrases.

    Defining the terms of artificial intelligence

    Defining the terms of artificial intelligence
  accessed:
    - year: 2020
      month: 6
      day: 21
  author:
    - family: Hutson
      given: Matthew
  container-title: Science
  DOI: 10.1126/science.357.6346.19
  ISSN: '0036-8075, 1095-9203'
  issue: '6346'
  issued:
    - year: 2017
      month: 7
      day: 7
  language: en
  page: 19-19
  PMID: '28684481'
  publisher: American Association for the Advancement of Science
  section: News
  source: science.sciencemag.org
  title: 'AI Glossary: Artificial intelligence, in so many words'
  title-short: AI Glossary
  type: article-journal
  URL: 'https://science.sciencemag.org/content/357/6346/19'
  volume: '357'

- id: IDSIASacred2020
  abstract: >-
    Sacred is a tool to help you configure, organize, log and reproduce
    experiments developed at IDSIA.
  accessed:
    - year: 2020
      month: 6
      day: 18
  genre: Python
  issued:
    - year: 2020
      month: 6
      day: 18
  original-date:
    - year: 2014
      month: 3
      day: 31
  publisher: IDSIA
  source: GitHub
  title: IDSIA/sacred
  type: book
  URL: 'https://github.com/IDSIA/sacred'

- id: IDSIASacred2020a
  abstract: >-
    Sacred is a tool to help you configure, organize, log and reproduce
    experiments developed at IDSIA.
  accessed:
    - year: 2020
      month: 6
      day: 22
  genre: Python
  issued:
    - year: 2020
      month: 6
      day: 22
  original-date:
    - year: 2014
      month: 3
      day: 31
  publisher: IDSIA
  source: GitHub
  title: IDSIA/sacred
  type: book
  URL: 'https://github.com/IDSIA/sacred'

- id: IterativeDvc2020
  abstract: "\U0001F989Data Version Control | Git for Data & Models. Contribute to iterative/dvc development by creating an account on GitHub."
  accessed:
    - year: 2020
      month: 6
      day: 18
  genre: Python
  issued:
    - year: 2020
      month: 6
      day: 18
  original-date:
    - year: 2017
      month: 3
      day: 4
  publisher: Iterative
  source: GitHub
  title: iterative/dvc
  type: book
  URL: 'https://github.com/iterative/dvc'

- id: kearns2013machine
  author:
    - family: Kearns
      given: Michael
    - family: Nevmyvaka
      given: Yuriy
  container-title: 'High Frequency Trading: New Realities for Traders, Markets, and Regulators'
  issued:
    - year: 2013
  title: Machine learning for market microstructure and high frequency trading
  type: article-journal

- id: keLightGBMHighlyEfficient2017
  accessed:
    - year: 2020
      month: 6
      day: 21
  author:
    - family: Ke
      given: Guolin
    - family: Meng
      given: Qi
    - family: Finley
      given: Thomas
    - family: Wang
      given: Taifeng
    - family: Chen
      given: Wei
    - family: Ma
      given: Weidong
    - family: Ye
      given: Qiwei
    - family: Liu
      given: Tie-Yan
  container-title: Advances in Neural Information Processing Systems 30
  editor:
    - family: Guyon
      given: I.
    - family: Luxburg
      given: U. V.
    - family: Bengio
      given: S.
    - family: Wallach
      given: H.
    - family: Fergus
      given: R.
    - family: Vishwanathan
      given: S.
    - family: Garnett
      given: R.
  issued:
    - year: 2017
  page: 3146–3154
  publisher: 'Curran Associates, Inc.'
  source: Neural Information Processing Systems
  title: 'LightGBM: A Highly Efficient Gradient Boosting Decision Tree'
  title-short: LightGBM
  type: chapter
  URL: >-
    http://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf

- id: kelleherDataScience2018
  accessed:
    - year: 2020
      month: 6
      day: 21
  author:
    - family: Kelleher
      given: John D.
    - family: Tierney
      given: Brendan
  DOI: 10.7551/mitpress/11140.001.0001
  ISBN: 978-0-262-34702-0
  issued:
    - year: 2018
  language: en
  publisher: The MIT Press
  source: DOI.org (Crossref)
  title: Data Science
  type: book
  URL: 'https://direct.mit.edu/books/book/3667/data-science'

- id: kelleherSTANDARDDATASCIENCE2018
  accessed:
    - year: 2020
      month: 6
      day: 21
  container-author:
    - family: Kelleher
      given: John D.
    - family: Tierney
      given: Brendan
  container-title: Data Science
  DOI: 10.7551/mitpress/11140.003.0009
  ISBN: 978-0-262-34702-0
  issued:
    - year: 2018
  language: en
  publisher: The MIT Press
  source: DOI.org (Crossref)
  title: STANDARD DATA SCIENCE TASKS
  type: chapter
  URL: >-
    https://direct.mit.edu/books/book/3667/chapter/122276/standard-data-science-tasks

- id: kingmaAutoEncodingVariationalBayes2014
  abstract: >-
    How can we perform efficient inference and learning in directed
    probabilistic models, in the presence of continuous latent variables with
    intractable posterior distributions, and large datasets? We introduce a
    stochastic variational inference and learning algorithm that scales to large
    datasets and, under some mild differentiability conditions, even works in
    the intractable case. Our contributions is two-fold. First, we show that a
    reparameterization of the variational lower bound yields a lower bound
    estimator that can be straightforwardly optimized using standard stochastic
    gradient methods. Second, we show that for i.i.d. datasets with continuous
    latent variables per datapoint, posterior inference can be made especially
    efficient by fitting an approximate inference model (also called a
    recognition model) to the intractable posterior using the proposed lower
    bound estimator. Theoretical advantages are reflected in experimental
    results.
  accessed:
    - year: 2020
      month: 6
      day: 22
  author:
    - family: Kingma
      given: Diederik P.
    - family: Welling
      given: Max
  container-title: 'arXiv:1312.6114 [cs, stat]'
  issued:
    - year: 2014
      month: 5
      day: 1
  source: arXiv.org
  title: Auto-Encoding Variational Bayes
  type: article-journal
  URL: 'http://arxiv.org/abs/1312.6114'

- id: kingmaIntroductionVariationalAutoencoders2019
  abstract: >-
    Variational autoencoders provide a principled framework for learning deep
    latent-variable models and corresponding inference models. In this work, we
    provide an introduction to variational autoencoders and some important
    extensions.
  accessed:
    - year: 2020
      month: 6
      day: 22
  author:
    - family: Kingma
      given: Diederik P.
    - family: Welling
      given: Max
  container-title: Foundations and Trends® in Machine Learning
  container-title-short: FNT in Machine Learning
  DOI: 10.1561/2200000056
  ISSN: '1935-8237, 1935-8245'
  issue: '4'
  issued:
    - year: 2019
  page: 307-392
  source: arXiv.org
  title: An Introduction to Variational Autoencoders
  type: article-journal
  URL: 'http://arxiv.org/abs/1906.02691'
  volume: '12'

- id: kraghQuantumGenerationsHistory2002
  abstract: >-
    At the end of the nineteenth century, some physicists believed that the
    basic principles underlying their subject were already known, and that
    physics in the future would only consist of filling in the details. They
    could hardly have been more wrong. The past century has seen the rise of
    quantum mechanics, relativity, cosmology, particle physics, and solid-state
    physics, among other fields. These subjects have fundamentally changed our
    understanding of space, time, and matter. They have also transformed daily
    life, inspiring a technological revolution that has included the development
    of radio, television, lasers, nuclear power, and computers. In Quantum
    Generations, Helge Kragh, one of the world's leading historians of physics,
    presents a sweeping account of these extraordinary achievements of the past
    one hundred years. The first comprehensive one-volume history of
    twentieth-century physics, the book takes us from the discovery of X rays in
    the mid-1890s to superstring theory in the 1990s. Unlike most previous
    histories of physics, written either from a scientific perspective or from a
    social and institutional perspective, Quantum Generations combines both
    approaches. Kragh writes about pure science with the expertise of a trained
    physicist, while keeping the content accessible to nonspecialists and paying
    careful attention to practical uses of science, ranging from compact disks
    to bombs. As a historian, Kragh skillfully outlines the social and economic
    contexts that have shaped the field in the twentieth century. He writes, for
    example, about the impact of the two world wars, the fate of physics under
    Hitler, Mussolini, and Stalin, the role of military research, the emerging
    leadership of the United States, and the backlash against science that began
    in the 1960s. He also shows how the revolutionary discoveries of scientists
    ranging from Einstein, Planck, and Bohr to Stephen Hawking have been built
    on the great traditions of earlier centuries. Combining a mastery of detail
    with a sure sense of the broad contours of historical change, Kragh has
    written a fitting tribute to the scientists who have played such a decisive
    role in the making of the modern world.
  author:
    - family: Kragh
      given: Helge
  ISBN: 978-0-691-09552-3
  issued:
    - year: 2002
      month: 3
      day: 24
  language: en
  number-of-pages: '514'
  publisher: Princeton University Press
  source: Google Books
  title: 'Quantum Generations: A History of Physics in the Twentieth Century'
  title-short: Quantum Generations
  type: book

- id: kruchtenTechnicalDebtMetaphor2012
  abstract: >-
    The metaphor of technical debt in software development was introduced two
    decades ago to explain to nontechnical stakeholders the need for what we
    call now "refactoring." As the term is being used to describe a wide range
    of phenomena, this paper proposes an organization of the technical debt
    landscape, and introduces the papers on technical debt contained in the
    issue.
  author:
    - family: Kruchten
      given: Philippe
    - family: Nord
      given: Robert L.
    - family: Ozkaya
      given: Ipek
  container-title: IEEE Software
  DOI: 10.1109/MS.2012.167
  ISSN: 1937-4194
  issue: '6'
  issued:
    - year: 2012
      month: 11
  page: 18-21
  source: IEEE Xplore
  title: 'Technical Debt: From Metaphor to Theory and Practice'
  title-short: Technical Debt
  type: article-journal
  volume: '29'

- id: Kubeflow
  abstract: >-
    Kubeflow makes deployment of ML Workflows on Kubernetes straightforward and
    automated
  accessed:
    - year: 2020
      month: 6
      day: 22
  container-title: Kubeflow
  language: en
  source: www.kubeflow.org
  title: Kubeflow
  type: webpage
  URL: 'https://www.kubeflow.org/'

- id: liHyperbandNovelBanditbased2017
  abstract: >-
    Performance of machine learning algorithms depends critically on identifying
    a good set of hyperparameters. While recent approaches use Bayesian
    optimization to adaptively select configurations, we focus on speeding up
    random search through adaptive resource allocation and early-stopping. We
    formulate hyperparameter optimization as a pure-exploration nonstochastic
    infinite-armed bandit problem where a predefined resource like iterations,
    data samples, or features is allocated to randomly sampled configurations.
    We introduce a novel algorithm, Hyperband, for this framework and analyze
    its theoretical properties, providing several desirable guarantees.
    Furthermore, we compare Hyperband with popular Bayesian optimization methods
    on a suite of hyperparameter optimization problems. We observe that
    Hyperband can provide over an order-of-magnitude speedup over our competitor
    set on a variety of deep-learning and kernel-based learning problems.
  author:
    - family: Li
      given: Lisha
    - family: Jamieson
      given: Kevin
    - family: DeSalvo
      given: Giulia
    - family: Rostamizadeh
      given: Afshin
    - family: Talwalkar
      given: Ameet
  container-title: The Journal of Machine Learning Research
  container-title-short: J. Mach. Learn. Res.
  ISSN: 1532-4435
  issue: '1'
  issued:
    - year: 2017
      month: 1
      day: 1
  page: 6765–6816
  source: January 2017
  title: 'Hyperband: a novel bandit-based approach to hyperparameter optimization'
  title-short: Hyperband
  type: article-journal
  volume: '18'

- id: luRecommenderSystems2012
  abstract: >-
    The ongoing rapid expansion of the Internet greatly increases the necessity
    of effective recommender systems for filtering the abundant information.
    Extensive research for recommender systems is conducted by a broad range of
    communities including social and computer scientists, physicists, and
    interdisciplinary researchers. Despite substantial theoretical and practical
    achievements, unification and comparison of different approaches are
    lacking, which impedes further advances. In this article, we review recent
    developments in recommender systems and discuss the major challenges. We
    compare and evaluate available algorithms and examine their roles in the
    future developments. In addition to algorithms, physical aspects are
    described to illustrate macroscopic behavior of recommender systems.
    Potential impacts and future directions are discussed. We emphasize that
    recommendation has great scientific depth and combines diverse research
    fields which makes it interesting for physicists as well as
    interdisciplinary researchers.
  accessed:
    - year: 2020
      month: 6
      day: 21
  author:
    - family: Lü
      given: Linyuan
    - family: Medo
      given: Matúš
    - family: Yeung
      given: Chi Ho
    - family: Zhang
      given: Yi-Cheng
    - family: Zhang
      given: Zi-Ke
    - family: Zhou
      given: Tao
  collection-title: Recommender Systems
  container-title: Physics Reports
  container-title-short: Physics Reports
  DOI: 10.1016/j.physrep.2012.02.006
  ISSN: 0370-1573
  issue: '1'
  issued:
    - year: 2012
      month: 10
      day: 1
  language: en
  page: 1-49
  source: ScienceDirect
  title: Recommender systems
  type: article-journal
  URL: 'http://www.sciencedirect.com/science/article/pii/S0370157312000828'
  volume: '519'

- id: MachinableorgMachinable2020
  abstract: A modular configuration system for machine learning research
  accessed:
    - year: 2020
      month: 6
      day: 18
  genre: Python
  issued:
    - year: 2020
      month: 6
      day: 17
  original-date:
    - year: 2019
      month: 11
      day: 19
  publisher: machinable
  source: GitHub
  title: machinable-org/machinable
  type: book
  URL: 'https://github.com/machinable-org/machinable'

- id: MachineLearningTrick2015
  abstract: "Our ability to rewrite statistical problems in an equivalent but different form, to reparameterise them, is one of the most general-purpose\_tools we have\_in mathematical statistics.\_We used reparam…"
  accessed:
    - year: 2020
      month: 6
      day: 18
  container-title: The Spectator
  issued:
    - year: 2015
      month: 10
      day: 29
  language: en-GB
  source: blog.shakirm.com
  title: 'Machine Learning Trick of the Day (4): Reparameterisation Tricks'
  title-short: Machine Learning Trick of the Day (4)
  type: post-weblog
  URL: >-
    http://blog.shakirm.com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks/

- id: merkel2014docker
  author:
    - family: Merkel
      given: Dirk
  container-title: Linux journal
  issue: '239'
  issued:
    - year: 2014
  page: '2'
  title: >-
    Docker: lightweight linux containers for consistent development and
    deployment
  type: article-journal
  volume: '2014'

- id: mesnardReproducibleWorkflowPublic2020
  abstract: >-
    In a new effort to make our research transparent and reproducible by others,
    we developed a workflow to run and share computational studies on the public
    cloud Microsoft Azure. It uses Docker containers to create an image of the
    application software stack. We also adopt several tools that facilitate
    creating and managing virtual machines on compute nodes and submitting jobs
    to these nodes. The configuration files for these tools are part of an
    expanded "reproducibility package" that includes workflow definitions for
    cloud computing, in addition to input files and instructions. This
    facilitates re-creating the cloud environment to re-run the computations
    under the same conditions. Although cloud providers have improved their
    offerings, many researchers using high-performance computing (HPC) are still
    skeptical about cloud computing. Thus, we ran benchmarks for tightly coupled
    applications to confirm that the latest HPC nodes of Microsoft Azure are
    indeed a viable alternative to traditional on-site HPC clusters. We also
    show that cloud offerings are now adequate to complete computational fluid
    dynamics studies with in-house research software that uses parallel
    computing with GPUs. Finally, we share with the community what we have
    learned from nearly two years of using Azure cloud to enhance transparency
    and reproducibility in our computational simulations.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Mesnard
      given: Olivier
    - family: Barba
      given: Lorena A.
  container-title: Computing in Science & Engineering
  container-title-short: Comput. Sci. Eng.
  DOI: 10.1109/MCSE.2019.2941702
  ISSN: '1521-9615, 1558-366X'
  issue: '1'
  issued:
    - year: 2020
      month: 1
      day: 1
  page: 102-116
  source: arXiv.org
  title: Reproducible Workflow on a Public Cloud for Computational Fluid Dynamics
  type: article-journal
  URL: 'http://arxiv.org/abs/1904.07981'
  volume: '22'

- id: MLOpsContinuousDelivery
  accessed:
    - year: 2020
      month: 6
      day: 21
  container-title: Google Cloud
  language: en
  source: cloud.google.com
  title: 'MLOps: Continuous delivery and automation pipelines in machine learning'
  title-short: MLOps
  type: webpage
  URL: >-
    https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning

- id: moritzRayDistributedFramework2018
  abstract: >-
    The next generation of AI applications will continuously interact with the
    environment and learn from these interactions. These applications impose new
    and demanding systems requirements, both in terms of performance and
    flexibility. In this paper, we consider these requirements and present
    Ray---a distributed system to address them. Ray implements a unified
    interface that can express both task-parallel and actor-based computations,
    supported by a single dynamic execution engine. To meet the performance
    requirements, Ray employs a distributed scheduler and a distributed and
    fault-tolerant store to manage the system's control state. In our
    experiments, we demonstrate scaling beyond 1.8 million tasks per second and
    better performance than existing specialized systems for several challenging
    reinforcement learning applications.
  accessed:
    - year: 2020
      month: 6
      day: 21
  author:
    - family: Moritz
      given: Philipp
    - family: Nishihara
      given: Robert
    - family: Wang
      given: Stephanie
    - family: Tumanov
      given: Alexey
    - family: Liaw
      given: Richard
    - family: Liang
      given: Eric
    - family: Elibol
      given: Melih
    - family: Yang
      given: Zongheng
    - family: Paul
      given: William
    - family: Jordan
      given: Michael I.
    - family: Stoica
      given: Ion
  container-title: 'arXiv:1712.05889 [cs, stat]'
  issued:
    - year: 2018
      month: 9
      day: 29
  source: arXiv.org
  title: 'Ray: A Distributed Framework for Emerging AI Applications'
  title-short: Ray
  type: article-journal
  URL: 'http://arxiv.org/abs/1712.05889'

- id: moritzRayDistributedFramework2018a
  abstract: >-
    The next generation of AI applications will continuously interact with the
    environment and learn from these interactions. These applications impose new
    and demanding systems requirements, both in terms of performance and
    flexibility. In this paper, we consider these requirements and present
    Ray---a distributed system to address them. Ray implements a unified
    interface that can express both task-parallel and actor-based computations,
    supported by a single dynamic execution engine. To meet the performance
    requirements, Ray employs a distributed scheduler and a distributed and
    fault-tolerant store to manage the system's control state. In our
    experiments, we demonstrate scaling beyond 1.8 million tasks per second and
    better performance than existing specialized systems for several challenging
    reinforcement learning applications.
  accessed:
    - year: 2020
      month: 6
      day: 24
  author:
    - family: Moritz
      given: Philipp
    - family: Nishihara
      given: Robert
    - family: Wang
      given: Stephanie
    - family: Tumanov
      given: Alexey
    - family: Liaw
      given: Richard
    - family: Liang
      given: Eric
    - family: Elibol
      given: Melih
    - family: Yang
      given: Zongheng
    - family: Paul
      given: William
    - family: Jordan
      given: Michael I.
    - family: Stoica
      given: Ion
  container-title: 'arXiv:1712.05889 [cs, stat]'
  issued:
    - year: 2018
      month: 9
      day: 29
  source: arXiv.org
  title: 'Ray: A Distributed Framework for Emerging AI Applications'
  title-short: Ray
  type: article-journal
  URL: 'http://arxiv.org/abs/1712.05889'

- id: nagarajanDeterministicImplementationsReproducibility2019
  abstract: >-
    While deep reinforcement learning (DRL) has led to numerous successes in
    recent years, reproducing these successes can be extremely challenging. One
    reproducibility challenge particularly relevant to DRL is nondeterminism in
    the training process, which can substantially affect the results. Motivated
    by this challenge, we study the positive impacts of deterministic
    implementations in eliminating nondeterminism in training. To do so, we
    consider the particular case of the deep Q-learning algorithm, for which we
    produce a deterministic implementation by identifying and controlling all
    sources of nondeterminism in the training process. One by one, we then allow
    individual sources of nondeterminism to affect our otherwise deterministic
    implementation, and measure the impact of each source on the variance in
    performance. We find that individual sources of nondeterminism can
    substantially impact the performance of agent, illustrating the benefits of
    deterministic implementations. In addition, we also discuss the important
    role of deterministic implementations in achieving exact replicability of
    results.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Nagarajan
      given: Prabhat
    - family: Warnell
      given: Garrett
    - family: Stone
      given: Peter
  container-title: 'arXiv:1809.05676 [cs]'
  issued:
    - year: 2019
      month: 6
      day: 9
  source: arXiv.org
  title: >-
    Deterministic Implementations for Reproducibility in Deep Reinforcement
    Learning
  type: article-journal
  URL: 'http://arxiv.org/abs/1809.05676'

- id: naglerSustainabilityReproducibilityContainerized2015
  abstract: >-
    Recent developments in the commercial open source community have catalysed
    the use of Linux containers for scalable deployment of web-based
    applications to the cloud. Scientific software can be containerized with
    dependencies, configuration files, post-processing tools and even simulation
    results, referred to as containerized computing. This new approach promises
    to significantly improve sustainability, productivity and reproducibility.
    We present our experiences, technology, and future plans for open source
    containerization of software used to model particle and radiation beams.
    Vagrant is central to our approach, using Docker for cloud deployment and
    VirtualBox virtual machines for deployment to Mac OS and Windows computers.
    Our technology enables seamless switching between the desktop and the cloud
    to simplify simulation development and execution.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Nagler
      given: Robert
    - family: Bruhwiler
      given: David
    - family: Moeller
      given: Paul
    - family: Webb
      given: Stephen
  container-title: 'arXiv:1509.08789 [cs]'
  issued:
    - year: 2015
      month: 9
      day: 28
  source: arXiv.org
  title: Sustainability and Reproducibility via Containerized Computing
  type: article-journal
  URL: 'http://arxiv.org/abs/1509.08789'

- id: nakandalaTamingModelServing
  abstract: >-
    Machine Learning (ML) adoption in the enterprise requires simpler and more
    efﬁcient software infrastructure—the bespoke solutions typical in large web
    companies are simply untenable. Model scoring, the process of obtaining
    prediction from a trained model over new data, is a primary contributor to
    infrastructure complexity and cost, as models are trained once but used many
    times.
  author:
    - family: Nakandala
      given: Supun
    - family: Saur
      given: Karla
    - family: Yu
      given: Gyeong-In
    - family: Karanasos
      given: Konstantinos
    - family: Curino
      given: Carlo
    - family: Weimer
      given: Markus
    - family: Interlandi
      given: Matteo
  language: en
  page: '16'
  source: Zotero
  title: >-
    Taming Model Serving Complexity, Performance and Cost: A Compilation to
    Tensor Computations Approach
  type: article-journal

- id: ng2011sparse
  author:
    - family: Ng
      given: Andrew
    - literal: others
  container-title: CS294A Lecture notes
  issue: '2011'
  issued:
    - year: 2011
  page: 1–19
  title: Sparse autoencoder
  type: article-journal
  volume: '72'

- id: nwankpaActivationFunctionsComparison2018
  abstract: >-
    Deep neural networks have been successfully used in diverse emerging domains
    to solve real world complex problems with may more deep learning(DL)
    architectures, being developed to date. To achieve these state-of-the-art
    performances, the DL architectures use activation functions (AFs), to
    perform diverse computations between the hidden layers and the output layers
    of any given DL architecture. This paper presents a survey on the existing
    AFs used in deep learning applications and highlights the recent trends in
    the use of the activation functions for deep learning applications. The
    novelty of this paper is that it compiles majority of the AFs used in DL and
    outlines the current trends in the applications and usage of these functions
    in practical deep learning deployments against the state-of-the-art research
    results. This compilation will aid in making effective decisions in the
    choice of the most suitable and appropriate activation function for any
    given application, ready for deployment. This paper is timely because most
    research papers on AF highlights similar works and results while this paper
    will be the first, to compile the trends in AF applications in practice
    against the research results from literature, found in deep learning
    research to date.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Nwankpa
      given: Chigozie
    - family: Ijomah
      given: Winifred
    - family: Gachagan
      given: Anthony
    - family: Marshall
      given: Stephen
  container-title: 'arXiv:1811.03378 [cs]'
  issued:
    - year: 2018
      month: 11
      day: 8
  source: arXiv.org
  title: >-
    Activation Functions: Comparison of trends in Practice and Research for Deep
    Learning
  title-short: Activation Functions
  type: article-journal
  URL: 'http://arxiv.org/abs/1811.03378'

- id: olorisadeReproducibilityMachineLearningBased2017
  abstract: >-
    Reproducibility is an essential requirement for computational studies
    including those based on machine learning techniques. However, many machine
    learning studies are either not reproducible or are difficult to reproduce.
    In this paper, we consider what information about text mining studies is
    crucial to successful reproduction of such studies. We identify a set of
    factors that affect reproducibility based on our experience of attempting to
    reproduce six studies proposing text mining techniques for the automation of
    the citation screening stage in the systematic review process. Subsequently,
    the reproducibility of 30 studies was evaluated based on the presence or
    otherwise of information relating to the factors. While the studies provide
    useful reports of their results, they lack information on access to the
    dataset in the form and order as used in the original study (as against raw
    data), the software environment used, randomization control and the
    implementation of proposed techniques. In order to increase the chances of
    being reproduced, researchers should ensure that details about and/or access
    to information about these factors are provided in their reports.
  author:
    - family: Olorisade
      given: Babatunde Kazeem
    - family: Brereton
      given: Pearl
    - family: Andras
      given: Peter
  issued:
    - year: 2017
  source: Semantic Scholar
  title: 'Reproducibility in Machine Learning-Based Studies: An Example of Text Mining'
  title-short: Reproducibility in Machine Learning-Based Studies
  type: paper-conference

- id: OnnxOnnx2020
  abstract: Open standard for machine learning interoperability
  accessed:
    - year: 2020
      month: 6
      day: 22
  genre: PureBasic
  issued:
    - year: 2020
      month: 6
      day: 22
  original-date:
    - year: 2017
      month: 9
      day: 7
  publisher: Open Neural Network Exchange
  source: GitHub
  title: onnx/onnx
  type: book
  URL: 'https://github.com/onnx/onnx'

- id: ostapchenkoQGSJETIIReliableDescription2006
  abstract: >-
    Since a number of years the QGSJET model has been successfully used by
    different groups in the field of high energy cosmic rays. Current work is
    devoted to the first general update of the model. The key improvement is
    connected to an account for non-linear interaction effects which are of
    crucial importance for reliable model extrapolation into the ultra-high
    energy domain. The proposed formalism allows to obtain a consistent
    description of hadron-hadron cross sections and hadron structure functions
    and to treat non-linear effects explicitly in individual hadronic and
    nuclear collisions. Other ameliorations concern the treatment of low mass
    diffraction, employment of realistic nuclear density profiles, and
    re-calibration of model parameters using a wider set of accelerator data.
  accessed:
    - year: 2020
      month: 6
      day: 25
  author:
    - family: Ostapchenko
      given: S.
  collection-title: VERY HIGH ENERGY COSMIC RAY INTERACTIONS
  container-title: Nuclear Physics B - Proceedings Supplements
  container-title-short: Nuclear Physics B - Proceedings Supplements
  DOI: 10.1016/j.nuclphysbps.2005.07.026
  ISSN: 0920-5632
  issue: '1'
  issued:
    - year: 2006
      month: 1
      day: 1
  language: en
  page: 143-146
  source: ScienceDirect
  title: >-
    QGSJET-II: towards reliable description of very high energy hadronic
    interactions
  title-short: QGSJET-II
  type: article-journal
  URL: 'http://www.sciencedirect.com/science/article/pii/S0920563205009175'
  volume: '151'

- id: pengReproducibilityCrisisScience2015
  abstract: >-
    More people have more access to data than ever before. But a comparative
    lack of analytical skills has resulted in scientific findings that are
    neither replicable nor reproducible. It is time to invest in statistics
    education, says Roger Peng
  accessed:
    - year: 2020
      month: 6
      day: 21
  author:
    - family: Peng
      given: Roger
  container-title: Significance
  DOI: 10.1111/j.1740-9713.2015.00827.x
  ISSN: 1740-9713
  issue: '3'
  issued:
    - year: 2015
  language: en
  note: >-
    _eprint:
    https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1740-9713.2015.00827.x
  page: 30-32
  source: Wiley Online Library
  title: 'The reproducibility crisis in science: A statistical counterattack'
  title-short: The reproducibility crisis in science
  type: article-journal
  URL: 'https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1740-9713.2015.00827.x'
  volume: '12'

- id: pengReproducibleResearchComputational2011
  abstract: >-
    Computational science has led to exciting new developments, but the nature
    of the work has exposed limitations in our ability to evaluate published
    findings. Reproducibility has the potential to serve as a minimum standard
    for judging scientific claims when full independent replication of a study
    is not possible.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Peng
      given: Roger D.
  container-title: Science
  DOI: 10.1126/science.1213847
  ISSN: '0036-8075, 1095-9203'
  issue: '6060'
  issued:
    - year: 2011
      month: 12
      day: 2
  language: en
  page: 1226-1227
  PMID: '22144613'
  publisher: American Association for the Advancement of Science
  section: Perspective
  source: science.sciencemag.org
  title: Reproducible Research in Computational Science
  type: article-journal
  URL: 'https://science.sciencemag.org/content/334/6060/1226'
  volume: '334'

- id: pierogEPOSLHCTest2015
  abstract: >-
    EPOS is a Monte-Carlo event generator for minimum bias hadronic
    interactions, used for both heavy ion interactions and cosmic ray air shower
    simulations. Since the last public release in 2009, the LHC experiments have
    provided a number of very interesting data sets comprising minimum bias p-p,
    p-Pb and Pb-Pb interactions. We describe the changes required to the model
    to reproduce in detail the new data available from LHC and the consequences
    in the interpretation of these data. In particular we discuss the effect of
    the collective hadronization in p-p scattering. A different parametrization
    of flow has been introduced in the case of a small volume with high density
    of thermalized matter (core) reached in p-p compared to large volume
    produced in heavy ion collisions. Both parametrizations depend only on the
    geometry and the amount of secondary particles entering in the core and not
    on the beam mass or energy. The transition between the two flow regimes can
    be tested with p-Pb data. EPOS LHC is able to reproduce all minimum bias
  accessed:
    - year: 2020
      month: 6
      day: 25
  author:
    - family: Pierog
      given: T.
    - family: Karpenko
      given: Iu
    - family: Katzy
      given: J. M.
    - family: Yatsenko
      given: E.
    - family: Werner
      given: K.
  container-title: Physical Review C
  container-title-short: Phys. Rev. C
  DOI: 10.1103/PhysRevC.92.034906
  ISSN: '0556-2813, 1089-490X'
  issue: '3'
  issued:
    - year: 2015
      month: 9
      day: 14
  page: 034906
  source: arXiv.org
  title: 'EPOS LHC : test of collective hadronization with LHC data'
  title-short: EPOS LHC
  type: article-journal
  URL: 'http://arxiv.org/abs/1306.0121'
  volume: '92'

- id: PolyaxonMachineLearning
  abstract: >-
    Get familiar with Polyaxon - Open source machine learning on Kubernetes,
    deep Learning on Kubernetes.
  accessed:
    - year: 2020
      month: 6
      day: 22
  container-title: Polyaxon
  language: en
  source: polyaxon.com
  title: Polyaxon - machine learning at scale
  type: webpage
  URL: 'https://polyaxon.com/'

- id: PolyaxonPolyaxon2020
  abstract: >-
    Cloud native machine learning automation platform. Contribute to
    polyaxon/polyaxon development by creating an account on GitHub.
  accessed:
    - year: 2020
      month: 6
      day: 18
  genre: Python
  issued:
    - year: 2020
      month: 6
      day: 18
  original-date:
    - year: 2016
      month: 12
      day: 26
  publisher: polyaxon
  source: GitHub
  title: polyaxon/polyaxon
  type: book
  URL: 'https://github.com/polyaxon/polyaxon'

- id: polyzotisDataLifecycleChallenges2018
  abstract: >-
    Machine learning has become an essential tool for gleaning knowledge from
    data and tackling a diverse set of computationally hard tasks. However, the
    accuracy of a machine learned model is deeply tied to the data that it is
    trained on. Designing and building robust processes and tools that make it
    easier to analyze, validate, and transform data that is fed into large-scale
    machine learning systems poses data management challenges. Drawn from our
    experience in developing data-centric infrastructure for a production
    machine learning platform at Google, we summarize some of the interesting
    research challenges that we encountered, and survey some of the relevant
    literature from the data management and machine learning communities.
    Specifically, we explore challenges in three main areas of focus - data
    understanding, data validation and cleaning, and data preparation. In each
    of these areas, we try to explore how different constraints are imposed on
    the solutions depending on where in the lifecycle of a model the problems
    are encountered and who encounters them.
  accessed:
    - year: 2020
      month: 6
      day: 22
  author:
    - family: Polyzotis
      given: Neoklis
    - family: Roy
      given: Sudip
    - family: Whang
      given: Steven Euijong
    - family: Zinkevich
      given: Martin
  container-title: ACM SIGMOD Record
  container-title-short: SIGMOD Rec.
  DOI: 10.1145/3299887.3299891
  ISSN: 0163-5808
  issue: '2'
  issued:
    - year: 2018
      month: 12
      day: 11
  page: 17–28
  source: June 2018
  title: 'Data Lifecycle Challenges in Production Machine Learning: A Survey'
  title-short: Data Lifecycle Challenges in Production Machine Learning
  type: article-journal
  URL: 'https://doi.org/10.1145/3299887.3299891'
  volume: '47'

- id: polyzotisDataManagementChallenges2017
  abstract: >-
    The tutorial discusses data-management issues that arise in the context of
    machine learning pipelines deployed in production. Informed by our own
    experience with such largescale pipelines, we focus on issues related to
    understanding, validating, cleaning, and enriching training data. The goal
    of the tutorial is to bring forth these issues, draw connections to prior
    work in the database literature, and outline the open research questions
    that are not addressed by prior art.
  accessed:
    - year: 2020
      month: 6
      day: 21
  author:
    - family: Polyzotis
      given: Neoklis
    - family: Roy
      given: Sudip
    - family: Whang
      given: Steven Euijong
    - family: Zinkevich
      given: Martin
  collection-title: SIGMOD '17
  container-title: Proceedings of the 2017 ACM International Conference on Management of Data
  DOI: 10.1145/3035918.3054782
  event-place: 'Chicago, Illinois, USA'
  ISBN: 978-1-4503-4197-4
  issued:
    - year: 2017
      month: 5
      day: 9
  page: 1723–1726
  publisher: Association for Computing Machinery
  publisher-place: 'Chicago, Illinois, USA'
  source: ACM Digital Library
  title: Data Management Challenges in Production Machine Learning
  type: paper-conference
  URL: 'https://doi.org/10.1145/3035918.3054782'

- id: poppComprehensiveSupportLifecycle
  author:
    - family: Popp
      given: Matthias
  language: en
  page: '71'
  source: Zotero
  title: >-
    Comprehensive Support of the Lifecycle of Machine Learning Models in Model
    Management Systems
  type: article-journal

- id: ProductionGradeContainerOrchestration
  abstract: Production-Grade Container Orchestration
  accessed:
    - year: 2020
      month: 6
      day: 22
  container-title: Kubernetes
  language: en
  source: kubernetes.io
  title: Production-Grade Container Orchestration
  type: webpage
  URL: 'https://kubernetes.io/'

- id: provost1998glossary
  author:
    - family: Provost
      given: Foster
    - family: Kohavi
      given: R
  container-title: Journal of Machine Learning
  issue: 2-3
  issued:
    - year: 1998
  page: 271–274
  title: Glossary of terms
  type: article-journal
  volume: '30'

- id: raffStepQuantifyingIndependently2019
  abstract: >-
    What makes a paper independently reproducible? Debates on reproducibility
    center around intuition or assumptions but lack empirical results. Our field
    focuses on releasing code, which is important, but is not sufficient for
    determining reproducibility. We take the first step toward a quantifiable
    answer by manually attempting to implement 255 papers published from 1984
    until 2017, recording features of each paper, and performing statistical
    analysis of the results. For each paper, we did not look at the authors
    code, if released, in order to prevent bias toward discrepancies between
    code and paper.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Raff
      given: Edward
  container-title: 'arXiv:1909.06674 [cs, stat]'
  issued:
    - year: 2019
      month: 9
      day: 14
  source: arXiv.org
  title: >-
    A Step Toward Quantifying Independently Reproducible Machine Learning
    Research
  type: article-journal
  URL: 'http://arxiv.org/abs/1909.06674'

- id: rampinReproZipReproducibilityPacker2016
  abstract: >-
    Rampin et al, (2016), ReproZip: The Reproducibility Packer, Journal of Open
    Source Software, 1(8), 107, doi:10.21105/joss.00107
  accessed:
    - year: 2020
      month: 6
      day: 22
  author:
    - family: Rampin
      given: Rémi
    - family: Chirigati
      given: Fernando
    - family: Shasha
      given: Dennis
    - family: Freire
      given: Juliana
    - family: Steeves
      given: Vicky
  container-title: Journal of Open Source Software
  DOI: 10.21105/joss.00107
  ISSN: 2475-9066
  issue: '8'
  issued:
    - year: 2016
      month: 12
      day: 1
  language: en
  page: '107'
  source: joss.theoj.org
  title: 'ReproZip: The Reproducibility Packer'
  title-short: ReproZip
  type: article-journal
  URL: 'https://joss.theoj.org/papers/10.21105/joss.00107'
  volume: '1'

- id: reOvertonDataSystem2019
  abstract: >-
    We describe a system called Overton, whose main design goal is to support
    engineers in building, monitoring, and improving production machine learning
    systems. Key challenges engineers face are monitoring fine-grained quality,
    diagnosing errors in sophisticated applications, and handling contradictory
    or incomplete supervision data. Overton automates the life cycle of model
    construction, deployment, and monitoring by providing a set of novel
    high-level, declarative abstractions. Overton's vision is to shift
    developers to these higher-level tasks instead of lower-level machine
    learning tasks. In fact, using Overton, engineers can build
    deep-learning-based applications without writing any code in frameworks like
    TensorFlow. For over a year, Overton has been used in production to support
    multiple applications in both near-real-time applications and back-of-house
    processing. In that time, Overton-based applications have answered billions
    of queries in multiple languages and processed trillions of records reducing
    errors 1.7-2.9 times versus production systems.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Ré
      given: Christopher
    - family: Niu
      given: Feng
    - family: Gudipati
      given: Pallavi
    - family: Srisuwananukorn
      given: Charles
  container-title: 'arXiv:1909.05372 [cs]'
  issued:
    - year: 2019
      month: 9
      day: 6
  source: arXiv.org
  title: 'Overton: A Data System for Monitoring and Improving Machine-Learned Products'
  title-short: Overton
  type: article-journal
  URL: 'http://arxiv.org/abs/1909.05372'

- id: sandveTenSimpleRules2013
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Sandve
      given: Geir Kjetil
    - family: Nekrutenko
      given: Anton
    - family: Taylor
      given: James
    - family: Hovig
      given: Eivind
  container-title: PLOS Computational Biology
  container-title-short: PLOS Computational Biology
  DOI: 10.1371/journal.pcbi.1003285
  ISSN: 1553-7358
  issue: '10'
  issued:
    - year: 2013
      month: 10
      day: 24
  language: en
  page: e1003285
  publisher: Public Library of Science
  source: PLoS Journals
  title: Ten Simple Rules for Reproducible Computational Research
  type: article-journal
  URL: >-
    https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003285
  volume: '9'

- id: schelterChallengesMachineLearning2018
  author:
    - family: Schelter
      given: Sebastian
    - family: Biessmann
      given: Felix
    - family: Januschowski
      given: Tim
    - family: Salinas
      given: David
    - family: Seufert
      given: Stephan
    - family: Szarvas
      given: Gyuri
    - family: Deshpande
      given: A
  container-title: IEEE Data Eng. Bull.
  issue: '4'
  issued:
    - year: 2018
  page: 5–15
  title: On challenges in machine learning model management.
  type: article-journal
  volume: '41'

- id: sculleyHiddenTechnicalDebt2015
  abstract: >-
    Machine learning offers a fantastically powerful toolkit for building useful
    complex prediction systems quickly. This paper argues it is dangerous to
    think of these quick wins as coming for free. Using the software engineering
    framework of technical debt, we find it is common to incur massive ongoing
    maintenance costs in real-world ML systems. We explore several ML-specific
    risk factors to account for in system design. These include boundary
    erosion, entanglement, hidden feedback loops, undeclared consumers, data
    dependencies, configuration issues, changes in the external world, and a
    variety of system-level anti-patterns.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Sculley
      given: D.
    - family: Holt
      given: Gary
    - family: Golovin
      given: Daniel
    - family: Davydov
      given: Eugene
    - family: Phillips
      given: Todd
    - family: Ebner
      given: Dietmar
    - family: Chaudhary
      given: Vinay
    - family: Young
      given: Michael
    - family: Crespo
      given: Jean-Francois
    - family: Dennison
      given: Dan
  collection-title: NIPS'15
  container-title: >-
    Proceedings of the 28th International Conference on Neural Information
    Processing Systems - Volume 2
  event-place: 'Montreal, Canada'
  issued:
    - year: 2015
      month: 12
      day: 7
  page: 2503–2511
  publisher: MIT Press
  publisher-place: 'Montreal, Canada'
  source: ACM Digital Library
  title: Hidden technical debt in Machine learning systems
  type: paper-conference

- id: sculleyMachineLearningHigh2014
  author:
    - family: Sculley
      given: D.
    - family: Holt
      given: Gary
    - family: Golovin
      given: Daniel
    - family: Davydov
      given: Eugene
    - family: Phillips
      given: Todd
    - family: Ebner
      given: Dietmar
    - family: Chaudhary
      given: Vinay
    - family: Young
      given: Michael
  container-title: 'SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop)'
  issued:
    - year: 2014
  source: Google Research
  title: 'Machine Learning: The High Interest Credit Card of Technical Debt'
  title-short: Machine Learning
  type: paper-conference

- id: seitaLearningLearn
  abstract: The BAIR Blog
  accessed:
    - year: 2020
      month: 6
      day: 22
  author:
    - family: Seita
      given: Daniel
  container-title: The Berkeley Artificial Intelligence Research Blog
  source: bair.berkeley.edu
  title: Learning to Learn
  type: webpage
  URL: 'http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/'

- id: shawahnaFPGABasedAcceleratorsDeep2019
  abstract: >-
    Due to recent advances in digital technologies, and availability of credible
    data, an area of artificial intelligence, deep learning, has emerged and has
    demonstrated its ability and effectiveness in solving complex learning
    problems not possible before. In particular, convolutional neural networks
    (CNNs) have demonstrated their effectiveness in the image detection and
    recognition applications. However, they require intensive CPU operations and
    memory bandwidth that make general CPUs fail to achieve the desired
    performance levels. Consequently, hardware accelerators that use
    application-specific integrated circuits, field-programmable gate arrays
    (FPGAs), and graphic processing units have been employed to improve the
    throughput of CNNs. More precisely, FPGAs have been recently adopted for
    accelerating the implementation of deep learning networks due to their
    ability to maximize parallelism and their energy efficiency. In this paper,
    we review the recent existing techniques for accelerating deep learning
    networks on FPGAs. We highlight the key features employed by the various
    techniques for improving the acceleration performance. In addition, we
    provide recommendations for enhancing the utilization of FPGAs for CNNs
    acceleration. The techniques investigated in this paper represent the recent
    trends in the FPGA-based accelerators of deep learning networks. Thus, this
    paper is expected to direct the future advances on efficient hardware
    accelerators and to be useful for deep learning researchers.
  author:
    - family: Shawahna
      given: Ahmad
    - family: Sait
      given: Sadiq M.
    - family: El-Maleh
      given: Aiman
  container-title: IEEE Access
  DOI: 10.1109/ACCESS.2018.2890150
  ISSN: 2169-3536
  issued:
    - year: 2019
  page: 7823-7859
  source: IEEE Xplore
  title: >-
    FPGA-Based Accelerators of Deep Learning Networks for Learning and
    Classification: A Review
  title-short: >-
    FPGA-Based Accelerators of Deep Learning Networks for Learning and
    Classification
  type: article-journal
  volume: '7'

- id: smithDisciplinedApproachNeural2018
  abstract: >-
    Although deep learning has produced dazzling successes for applications of
    image, speech, and video processing in the past few years, most trainings
    are with suboptimal hyper-parameters, requiring unnecessarily long training
    times. Setting the hyper-parameters remains a black art that requires years
    of experience to acquire. This report proposes several efficient ways to set
    the hyper-parameters that significantly reduce training time and improves
    performance. Specifically, this report shows how to examine the training
    validation/test loss function for subtle clues of underfitting and
    overfitting and suggests guidelines for moving toward the optimal balance
    point. Then it discusses how to increase/decrease the learning rate/momentum
    to speed up training. Our experiments show that it is crucial to balance
    every manner of regularization for each dataset and architecture. Weight
    decay is used as a sample regularizer to show how its optimal value is
    tightly coupled with the learning rates and momentums. Files to help
    replicate the results reported here are available.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Smith
      given: Leslie N.
  container-title: 'arXiv:1803.09820 [cs, stat]'
  issued:
    - year: 2018
      month: 4
      day: 24
  source: arXiv.org
  title: >-
    A disciplined approach to neural network hyper-parameters: Part 1 --
    learning rate, batch size, momentum, and weight decay
  title-short: A disciplined approach to neural network hyper-parameters
  type: article-journal
  URL: 'http://arxiv.org/abs/1803.09820'

- id: stanevHighEnergyCosmic2010
  abstract: >-
    Cosmic rays are an essential part of the universe. Their origin is related
    to many important astrophysical processes, such as star formation, stellar
    evolution, supernova explosions and the state of interstellar matter in the
    Galaxy. Cosmic Ray Physics reviews our present knowledge of cosmic rays,
    describing how they are born in a wide range of cosmic processes, how they
    are accelerated and how they interact with matter, magnetic fields and
    radiation during their journey across the Galaxy. The book also describes
    the detection of cosmic rays, and the processes which take place, both at
    the top and within the Earth’s atmosphere. The author also describes the
    very important area of the underground detection of very high energy cosmic
    rays and particles such as neutrinos. The book is divided into two parts,
    the first describing the standard model of cosmic rays and contemporary
    challenges, and the second part dealing with very high energy cosmic rays
    that cannot be detected directly in satellite and balloon experiments, and
    with gamma-ray and neutrino astronomy. It is in this particular aspect of
    the book that the greatest developments have taken place during the 5 years
    since the first edition was completed. Consequently, it is in the chapters
    cosmic ray showers, their spectrum, on high energy neutrinos, and on
    gamma-ray astronomy of this revised and updated 2nd edition that a
    considerable amount of new material has been incorporated with more minor
    revisions and updating taking place in the first part of the book. Students
    and lecturers of advanced undergraduate courses on cosmic rays and
    astroparticle physics as well as post graduates and researchers will
    continue to find this book a valuable source of learning and reference.
  author:
    - family: Stanev
      given: Todor
  ISBN: 978-3-540-85148-6
  issued:
    - year: 2010
      month: 3
      day: 10
  language: en
  number-of-pages: '333'
  publisher: Springer Science & Business Media
  source: Google Books
  title: High Energy Cosmic Rays
  type: book

- id: stanevOverview2010
  abstract: >-
    Cosmic rays are an essential part of the universe. Their origin is related
    to many important astrophysical processes, such as star formation, stellar
    evolution, supernova explosions and the state of interstellar matter in the
    Galaxy. Cosmic Ray Physics reviews our present knowledge of cosmic rays,
    describing how they are born in a wide range of cosmic processes, how they
    are accelerated and how they interact with matter, magnetic fields and
    radiation during their journey across the Galaxy. The book also describes
    the detection of cosmic rays, and the processes which take place, both at
    the top and within the Earth’s atmosphere. The author also describes the
    very important area of the underground detection of very high energy cosmic
    rays and particles such as neutrinos. The book is divided into two parts,
    the first describing the standard model of cosmic rays and contemporary
    challenges, and the second part dealing with very high energy cosmic rays
    that cannot be detected directly in satellite and balloon experiments, and
    with gamma-ray and neutrino astronomy. It is in this particular aspect of
    the book that the greatest developments have taken place during the 5 years
    since the first edition was completed. Consequently, it is in the chapters
    cosmic ray showers, their spectrum, on high energy neutrinos, and on
    gamma-ray astronomy of this revised and updated 2nd edition that a
    considerable amount of new material has been incorporated with more minor
    revisions and updating taking place in the first part of the book. Students
    and lecturers of advanced undergraduate courses on cosmic rays and
    astroparticle physics as well as post graduates and researchers will
    continue to find this book a valuable source of learning and reference.
  author:
    - family: Stanev
      given: Todor
  container-title: High Energy Cosmic Rays
  ISBN: 978-3-540-85148-6
  issued:
    - year: 2010
      month: 3
      day: 10
  language: en
  publisher: Springer Science & Business Media
  source: Google Books
  title: Overview
  type: chapter

- id: stoddenSettingDefaultReproducible2013
  abstract: >-
    Science is built upon foundations of theory and experiment validated and
    improved through open, transparent communication. With the increasingly
    central role of computation in scientific discovery this means communicating
    all details of the computations needed for others to replicate the
    experiment, i.e. making available to others the associated data and code.
    The “reproducible research” movement recognizes that traditional scientific
    research and publication practices now fall short of this ideal, and
    encourages all those involved in the production of computational science ‐
    scientists who use computational methods and the institutions that employ
    them, journals and dissemination mechanisms, and funding agencies ‐ to
    facilitate and practice really reproducible research. This report summarizes
    discussions that took place during the ICERM Workshop on Reproducibility in
    Computational and Experimental Mathematics, held December 10-14, 2012. The
    main recommendations that emerged from the workshop discussions are: 1. It
    is important to promote a culture change that will integrate computational
    reproducibility into the research process.
  accessed:
    - year: 2020
      month: 6
      day: 21
  author:
    - family: Stodden
      given: Victoria
    - family: Bailey
      given: David H.
    - family: Borwein
      given: Jonathan M.
    - family: LeVeque
      given: Randall J.
    - family: Rider
      given: William J.
    - family: Stein
      given: William
  container-title: undefined
  issued:
    - year: 2013
  language: en
  source: www.semanticscholar.org
  title: >-
    Setting the Default to Reproducible Reproducibility in Computational and
    Experimental Mathematics
  type: webpage
  URL: >-
    /paper/Setting-the-Default-to-Reproducible-Reproducibility-Stodden-Bailey/992647adcc7e3626768841acb039d2b4a70d5c95

- id: sutton1998introduction
  author:
    - family: Sutton
      given: Richard S
    - family: Barto
      given: Andrew G
    - literal: others
  issued:
    - year: 1998
  publisher: MIT press Cambridge
  title: Introduction to reinforcement learning
  type: book
  volume: '135'

- id: trippiArtificialIntelligenceFinance1995
  abstract: >-
    From the Publisher: In Artificial Intelligence in Finance and Investing,
    authors Robert Trippi and Jae Lee explain this fascinating new technology in
    terms that portfolio managers, institutional investors, investment analysis,
    and information systems professionals can understand. Using real-life
    examples and a practical approach, this rare and readable volume discusses
    the entire field of artificial intelligence of relevance to investing, so
    that readers can realize the benefits and evaluate the features of existing
    or proposed systems, and ultimately construct their own systems. Topics
    include using Expert Systems for Asset Allocation, Timing Decisions, Pattern
    Recognition, and Risk Assessment; overview of Popular Knowledge-Based
    Systems; construction of Synergistic Rule Bases for Securities Selection;
    incorporating the Markowitz Portfolio Optimization Model into
    Knowledge-Based Systems; Bayesian Theory and Fuzzy Logic System Components;
    Machine Learning in Portfolio Selection and Investment Timing, including
    Pattern-Based Learning and Fenetic Algorithms; and Neural Network-Based
    Systems. To illustrate the concepts presented in the book, the authors
    conclude with a valuable practice session and analysis of a typical
    knowledge-based system for investment management, K-FOLIO. For those who
    want to stay on the cutting edge of the "application" revolution, Artificial
    Intelligence in Finance and Investing offers a pragmatic introduction to the
    use of knowledge-based systems in securities selection and portfolio
    management.
  author:
    - family: Trippi
      given: Robert R.
    - family: Lee
      given: Jae K.
  edition: 1st
  event-place: USA
  ISBN: 978-1-55738-868-1
  issued:
    - year: 1995
  number-of-pages: '246'
  publisher: 'McGraw-Hill, Inc.'
  publisher-place: USA
  source: ACM Digital Library
  title: >-
    Artificial Intelligence in Finance and Investing: State-of-the-Art
    Technologies for Securities Selection and Portfolio Management
  title-short: Artificial Intelligence in Finance and Investing
  type: book

- id: vincentConnectionScoreMatching2011
  abstract: >-
    Denoising autoencoders have been previously shown to be competitive
    alternatives to restricted Boltzmann machines for unsupervised pretraining
    of each layer of a deep architecture. We show that a simple denoising
    autoencoder training criterion is equivalent to matching the score (with
    respect to the data) of a specific energy-based model to that of a
    nonparametric Parzen density estimator of the data. This yields several
    useful insights. It defines a proper probabilistic model for the denoising
    autoencoder technique, which makes it in principle possible to sample from
    them or rank examples by their energy. It suggests a different way to apply
    score matching that is related to learning to denoise and does not require
    computing second derivatives. It justifies the use of tied weights between
    the encoder and decoder and suggests ways to extend the success of denoising
    autoencoders to a larger family of energy-based models.
  accessed:
    - year: 2020
      month: 6
      day: 22
  author:
    - family: Vincent
      given: Pascal
  container-title: Neural Computation
  DOI: 10.1162/NECO_a_00142
  ISSN: 0899-7667
  issue: '7'
  issued:
    - year: 2011
      month: 4
      day: 14
  page: 1661-1674
  publisher: MIT Press
  source: MIT Press Journals
  title: A Connection Between Score Matching and Denoising Autoencoders
  type: article-journal
  URL: 'https://doi.org/10.1162/NECO_a_00142'
  volume: '23'

- id: wangAutoencoderBasedDimensionality2016
  abstract: >-
    Auto-encoder—a tricky three-layered neural network, known as
    auto-association before, constructs the “building block” of deep learning,
    which has been demonstrated to achieve good performance in various domains.
    In this paper, we try to investigate the dimensionality reduction ability of
    auto-encoder, and see if it has some kind of good property that might
    accumulate when being stacked and thus contribute to the success of deep
    learning. Based on the above idea, this paper starts from auto-encoder and
    focuses on its ability to reduce the dimensionality, trying to understand
    the difference between auto-encoder and state-of-the-art dimensionality
    reduction methods. Experiments are conducted both on the synthesized data
    for an intuitive understanding of the method, mainly on two and
    three-dimensional spaces for better visualization, and on some real
    datasets, including MNIST and Olivetti face datasets. The results show that
    auto-encoder can indeed learn something different from other methods.
    Besides, we preliminarily investigate the influence of the number of hidden
    layer nodes on the performance of auto-encoder and its possible relation
    with the intrinsic dimensionality of input data.
  accessed:
    - year: 2020
      month: 6
      day: 22
  author:
    - family: Wang
      given: Yasi
    - family: Yao
      given: Hongxun
    - family: Zhao
      given: Sicheng
  collection-title: 'RoLoD: Robust Local Descriptors for Computer Vision 2014'
  container-title: Neurocomputing
  container-title-short: Neurocomputing
  DOI: 10.1016/j.neucom.2015.08.104
  ISSN: 0925-2312
  issued:
    - year: 2016
      month: 4
      day: 5
  language: en
  page: 232-242
  source: ScienceDirect
  title: Auto-encoder based dimensionality reduction
  type: article-journal
  URL: 'http://www.sciencedirect.com/science/article/pii/S0925231215017671'
  volume: '184'

- id: wardenBigDataGlossary2011
  abstract: >-
    To help you navigate the large number of new data tools available, this
    guide describes 60 of the most recent innovations, from NoSQL databases and
    MapReduce approaches to machine learning and visualization tools.
    Descriptions are based on first-hand experience with these tools in a
    production environment. This handy glossary also includes a chapter of key
    terms that help define many of these tool categories:NoSQL
    DatabasesDocument-oriented databases using a key/value interface rather than
    SQL MapReduceTools that support distributed computing on large datasets
    StorageTechnologies for storing data in a distributed way ServersWays to
    rent computing power on remote machines ProcessingTools for extracting
    valuable information from large datasets Natural Language ProcessingMethods
    for extracting information from human-created text Machine LearningTools
    that automatically perform data analyses, based on results of a one-off
    analysis VisualizationApplications that present meaningful data graphically
    AcquisitionTechniques for cleaning up messy public data sources
    SerializationMethods to convert data structure or object state into a
    storable format
  author:
    - family: Warden
      given: Pete
  ISBN: 978-1-4493-1459-0
  issued:
    - year: 2011
  number-of-pages: '60'
  publisher: 'O''Reilly Media, Inc.'
  source: ACM Digital Library
  title: Big Data Glossary
  type: book

- id: wilsonGoodEnoughPractices2017
  abstract: >-
    Author summary Computers are now essential in all branches of science, but
    most researchers are never taught the equivalent of basic lab skills for
    research computing. As a result, data can get lost, analyses can take much
    longer than necessary, and researchers are limited in how effectively they
    can work with software and data. Computing workflows need to follow the same
    practices as lab projects and notebooks, with organized data, documented
    steps, and the project structured for reproducibility, but researchers new
    to computing often don't know where to start. This paper presents a set of
    good computing practices that every researcher can adopt, regardless of
    their current level of computational skill. These practices, which encompass
    data management, programming, collaborating with colleagues, organizing
    projects, tracking work, and writing manuscripts, are drawn from a wide
    variety of published sources from our daily lives and from our work with
    volunteer organizations that have delivered workshops to over 11,000 people
    since 2010.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Wilson
      given: Greg
    - family: Bryan
      given: Jennifer
    - family: Cranston
      given: Karen
    - family: Kitzes
      given: Justin
    - family: Nederbragt
      given: Lex
    - family: Teal
      given: Tracy K.
  container-title: PLOS Computational Biology
  container-title-short: PLOS Computational Biology
  DOI: 10.1371/journal.pcbi.1005510
  ISSN: 1553-7358
  issue: '6'
  issued:
    - year: 2017
      month: 6
      day: 22
  language: en
  page: e1005510
  publisher: Public Library of Science
  source: PLoS Journals
  title: Good enough practices in scientific computing
  type: article-journal
  URL: >-
    https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510
  volume: '13'

- id: zahariaAcceleratingMachineLearning2018
  abstract: >-
    Machine learning development creates multiple new challenges that are not
    present in a traditional software development lifecycle. These include
    keeping track of the myriad inputs to an ML application (e.g., data
    versions, code and tuning parameters), reproducing results, and production
    deployment. In this paper, we summarize these challenges from our experience
    with Databricks customers, and describe MLflow, an open source platform we
    recently launched to streamline the machine learning lifecycle. MLflow
    covers three key challenges: experimentation, reproducibility, and model
    deployment, using generic APIs that work with any ML library, algorithm and
    programming language. The project has a rapidly growing open source
    community, with over 50 contributors since its launch in June 2018.
  author:
    - family: Zaharia
      given: Matei
    - family: Chen
      given: Andrew
    - family: Davidson
      given: Aaron
    - family: Ghodsi
      given: Ali
    - family: Hong
      given: Sue Ann
    - family: Konwinski
      given: Andy
    - family: Murching
      given: Siddharth
    - family: Nykodym
      given: Tomas
    - family: Ogilvie
      given: Paul
    - family: Parkhe
      given: Mani
    - family: Xie
      given: Fen
    - family: Zumar
      given: Corey
  container-title: IEEE Data Eng. Bull.
  issued:
    - year: 2018
  source: Semantic Scholar
  title: Accelerating the Machine Learning Lifecycle with MLflow
  type: article-journal

- id: zhangVELCNewVariational2020
  abstract: >-
    Anomaly detection is a classical but worthwhile problem, and many deep
    learning-based anomaly detection algorithms have been proposed, which can
    usually achieve better detection results than traditional methods. In view
    of reconstruct ability of the model and the calculation of anomaly score,
    this paper proposes a time series anomaly detection method based on
    Variational AutoEncoder model(VAE) with re-Encoder and Latent Constraint
    network(VELC). In order to modify reconstruct ability of the model to
    prevent it from reconstructing abnormal samples well, we add a constraint
    network in the latent space of the VAE to force it generate new latent
    variables that are similar with that of training samples. To be able to
    calculate anomaly score in two feature spaces, we train a re-encoder to
    transform the generated data to a new latent space. For better handling the
    time series, we use the LSTM as the encoder and decoder part of the VAE
    framework. Experimental results of several benchmarks show that our method
    outperforms state-of-the-art anomaly detection methods.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Zhang
      given: Chunkai
    - family: Li
      given: Shaocong
    - family: Zhang
      given: Hongye
    - family: Chen
      given: Yingyang
  container-title: 'arXiv:1907.01702 [cs, stat]'
  issued:
    - year: 2020
      month: 4
      day: 15
  source: arXiv.org
  title: >-
    VELC: A New Variational AutoEncoder Based Model for Time Series Anomaly
    Detection
  title-short: VELC
  type: article-journal
  URL: 'http://arxiv.org/abs/1907.01702'

- id: zhaoReproducingScientificExperiment2019
  abstract: >-
    The reproducibility of scientific experiment is vital for the advancement of
    disciplines based on previous work. To achieve this goal, many researchers
    focus on complex methodology and self-invented tools which have difficulty
    in practical usage. In this article, we introduce the DevOps infrastructure
    from software engineering community and shows how DevOps can be used
    effectively to reproduce experiments for computer science related
    disciplines. DevOps can be enabled using freely available cloud computing
    machines for medium sized experiment and self-hosted computing engines for
    large scale computing, thus powering researchers to share their experiment
    result with others in a more reliable way.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Zhao
      given: Feng
    - family: Niu
      given: Xingzhi
    - family: Huang
      given: Shao-Lun
    - family: Zhang
      given: Lin
  container-title: 'arXiv:1910.13397 [cs]'
  issued:
    - year: 2019
      month: 10
      day: 28
  source: arXiv.org
  title: Reproducing Scientific Experiment with Cloud DevOps
  type: article-journal
  URL: 'http://arxiv.org/abs/1910.13397'

- id: zhuClassificationSupervisedAutoEncoder2020
  abstract: >-
    Classic variational autoencoders are used to learn complex data
    distributions, that are built on standard function approximators.
    Especially, VAE has shown promise on a lot of complex task. In this paper, a
    new autoencoder model - classification supervised autoencoder (CSAE) based
    on predefined evenly-distributed class centroids (PEDCC) is proposed. Our
    method uses PEDCC of latent variables to train the network to ensure the
    maximization of inter-class distance and the minimization of inner-class
    distance. Instead of learning mean/variance of latent variables distribution
    and taking reparameterization of VAE, latent variables of CSAE are directly
    used to classify and as input of decoder. In addition, a new loss function
    is proposed to combine the loss function of classification. Based on the
    basic structure of the universal autoencoder, we realized the comprehensive
    optimal results of encoding, decoding, classification, and good model
    generalization performance at the same time. Theoretical advantages are
    reflected in experimental results.
  accessed:
    - year: 2020
      month: 6
      day: 18
  author:
    - family: Zhu
      given: Qiuyu
    - family: Zhang
      given: Ruixin
  container-title: 'arXiv:1902.00220 [cs]'
  issued:
    - year: 2020
      month: 1
      day: 10
  source: arXiv.org
  title: >-
    A Classification Supervised Auto-Encoder Based on Predefined
    Evenly-Distributed Class Centroids
  type: article-journal
  URL: 'http://arxiv.org/abs/1902.00220'
...
